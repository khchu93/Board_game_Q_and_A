Paper: Lost in the Middle: How Language Models Use Long Contexts
Q: When I retrieve 5 chunks for my RAG system, does it matter what order I put them in?
A: YES! The paper shows LLMs perform ~20% worse on information in the middle of context vs. beginning/end. For RAG, put your most relevant chunks at the START and END of your prompt. If you have 5 chunks ranked by relevance: [1st, 5th, 4th, 3rd, 2nd] - sandwich low-importance chunks in the middle.

Q: What about longer contexts? If I use GPT-4 with 128k context window, can I just dump all my documents in there?
A: No! Even with huge context windows, models still struggle with middle content. The paper tested up to 32k tokens and found the U-shaped pattern persists. More context â‰  better performance. Strategy: Be selective about what you include and carefully position critical information at edges.

Q: How much worse is the middle really? Is it noticeable?
A: Very noticeable! The paper shows accuracy can drop from ~90% for information at the start to ~70% in the middle, then back up to ~85% at the end. That's a 20+ percentage point swing just from position. For production systems, this is a critical performance hit.

Q: Does this happen with all models or just GPT?
A: All models tested! The paper evaluated GPT-3.5, GPT-4, Claude, and open-source models. They all show the U-shaped performance curve. This appears to be a fundamental limitation of how causal language models are trained, not specific to one provider.

Q: What if I have 10 chunks? Where should I put them?
A: Rank by relevance, then arrange: [1st, 10th, 9th, 4th, 5th, 6th, 7th, 8th, 3rd, 2nd]. Put your best chunks at positions 1, 2, 9, 10. Put your worst chunks in positions 4-7 (the dead zone). This maximizes the chance your important information gets used.

Q: Can I just tell the model "pay attention to the middle"?
A: The paper tested this - it doesn't work well. You can add instructions like "carefully read all information" but models still show position bias. The issue seems to be in the attention mechanism itself, not just following instructions. Better to work with the bias than fight it.

Q: What's the minimum context length where this matters?
A: The effect appears around 2k+ tokens. For very short contexts (<1k tokens), position matters less. But once you're retrieving 3+ decent-sized chunks (300-500 tokens each), you're in the danger zone and need to think about positioning.

Q: Does this apply to my prompt instructions too?
A: Yes! If you have a long system prompt, put your most important instructions at the beginning and end. Don't bury critical guidelines in the middle of a 1000-word instruction set. Many people put key rules at the top as a header for this reason.

Q: What about question-answering? Should I put the question at the start or end?
A: The paper suggests putting the question at the END after all context. This mirrors how humans read - consume information, then answer the question. Pattern: [Context chunks positioned strategically] + [Question at end].

Q: Is there a sweet spot for number of chunks?
A: 3-5 chunks is optimal. With 3 chunks, you have: [important] [less important] [important]. With 5: [best] [good] [mediocre] [good] [best]. Beyond 5-7 chunks, you create a larger "dead zone" in the middle that will be ignored.