{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c114b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation System for Board Game Manuals\n",
    "============================================\n",
    "A retrieval-augmented generation (RAG) system with coverage-based evaluation.\n",
    "\n",
    "Key Features:\n",
    "- PDF text extraction with normalization\n",
    "- Ground truth Q&A annotation integration using Aho-Corasick pattern matching\n",
    "- Coverage-based relevance scoring (measures how much of a relevant span is in a chunk)\n",
    "- DCG/nDCG metrics for retrieval quality evaluation\n",
    "\n",
    "Adapted from: \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "Author: [Your Name]\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "# import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import ahocorasick\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import Dataset\n",
    "\n",
    "# import gc\n",
    "\n",
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa7b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class RAGEvaluationError(Exception):\n",
    "    \"\"\"Base exception for RAG evaluation system\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoadError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document loading fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AnnotationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when Q&A annotation processing fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ChunkingError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document chunking fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class VectorStoreError(RAGEvaluationError):\n",
    "    \"\"\"Raised when vector store operations fail\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EvaluationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when metric calculation fails\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c40c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to handle encoding inconsistencies between PDF and JSON.\n",
    "    \n",
    "    This is critical because:\n",
    "    - PDFs may have curly quotes/apostrophes: \"\", '', '\n",
    "    - JSON files typically use straight quotes: \", '\n",
    "    - Mismatches break pattern matching for ground truth annotation\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text with standardized quotes and collapsed whitespace\n",
    "    \"\"\"\n",
    "    # Convert curly quotes to straight quotes\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    \n",
    "    # Collapse all whitespace (newlines, tabs, multiple spaces) to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF and clean text content.\n",
    "    \n",
    "    Why cleaning matters:\n",
    "    - PDFs often have inconsistent spacing/newlines\n",
    "    - Normalized text improves embedding quality\n",
    "    - Standardized format makes pattern matching reliable\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the board game manual PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects (one per page) with cleaned text\n",
    "        \n",
    "    Raises:\n",
    "        DocumentLoadError: If PDF cannot be loaded or is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not Path(pdf_path).exists():\n",
    "            raise DocumentLoadError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading PDF from: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        if not page_docs:\n",
    "            raise DocumentLoadError(f\"No content extracted from PDF: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(page_docs)} pages from PDF\")\n",
    "        \n",
    "        # Clean text and filter metadata\n",
    "        for page_doc in page_docs:\n",
    "            # Normalize whitespace\n",
    "            clean_text = normalize_text(page_doc.page_content)\n",
    "            page_doc.page_content = clean_text\n",
    "            \n",
    "            # Keep only essential metadata to avoid Chroma serialization issues\n",
    "            allowed_keys = {\"source\", \"page\"}\n",
    "            page_doc.metadata = {\n",
    "                k: v for k, v in page_doc.metadata.items() \n",
    "                if k in allowed_keys\n",
    "            }\n",
    "        \n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, DocumentLoadError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading PDF: {str(e)}\")\n",
    "        raise DocumentLoadError(f\"Failed to load PDF: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35a7e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNKS STORE & RETRIEVE\n",
    "# =============================================================================\n",
    "\n",
    "def save_chunks(chunks, path):\n",
    "    serializable = [\n",
    "        {\n",
    "            \"content\": c.page_content,\n",
    "            \"metadata\": c.metadata\n",
    "        }\n",
    "        for c in chunks\n",
    "    ]\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_saved_chunks(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    chunks = [\n",
    "        Document(page_content=item[\"content\"], metadata=item[\"metadata\"])\n",
    "        for item in raw\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "086959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GROUND TRUTH ANNOTATION INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON file containing training Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON data\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If file cannot be loaded or parsed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(json_path).exists():\n",
    "            raise AnnotationError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded JSON from: {json_path}\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to parse JSON: {str(e)}\") from e\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading JSON: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to load JSON: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def load_training_qa_to_docs(training_qas_path: str, page_docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Annotate documents with ground truth relevance spans using Aho-Corasick.\n",
    "    \n",
    "    Why Aho-Corasick?\n",
    "    - Efficient multi-pattern matching: O(n + m + z) vs O(n*m) for naive search\n",
    "    - n = document length, m = total pattern length, z = matches\n",
    "    - Critical when searching 100+ patterns across large documents\n",
    "    \n",
    "    Process:\n",
    "    1. Build automaton with all relevant chunks from training Q&A\n",
    "    2. Scan each page once to find all matching spans\n",
    "    3. Store span metadata (qa_id, page, start/end indices)\n",
    "    \n",
    "    Args:\n",
    "        training_qas_path: Path to JSON with training Q&A pairs\n",
    "        page_docs: List of Document objects from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Documents annotated with relevance_spans in metadata\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If annotation process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = load_json(training_qas_path)\n",
    "        training_qas = training_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not training_qas:\n",
    "            logger.warning(\"No training Q&As found in JSON\")\n",
    "            return page_docs\n",
    "        \n",
    "        logger.info(f\"Processing {len(training_qas)} training Q&A pairs\")\n",
    "        \n",
    "        # Build Aho-Corasick automaton for efficient pattern matching\n",
    "        automaton = ahocorasick.Automaton()\n",
    "        \n",
    "        for qa_idx, qa in enumerate(training_qas):\n",
    "            qa[\"relevance_spans\"] = []  # Initialize spans list\n",
    "            \n",
    "            for chunk_text in qa.get(\"relevant_chunks\", []):\n",
    "                chunk_text_normalized = normalize_text(chunk_text)\n",
    "                \n",
    "                # Store tuple: (qa_index, original_chunk_text)\n",
    "                # qa_index allows us to map back to the question\n",
    "                automaton.add_word(chunk_text_normalized, (qa_idx, chunk_text_normalized))\n",
    "        \n",
    "        automaton.make_automaton()  # Compile the automaton\n",
    "        logger.info(\"Aho-Corasick automaton built successfully\")\n",
    "        \n",
    "        # Search all pages for relevant spans\n",
    "        total_spans = 0\n",
    "        for page_doc in page_docs:\n",
    "            page_text = normalize_text(page_doc.page_content)\n",
    "            page_num = page_doc.metadata.get(\"page\")\n",
    "            page_doc.metadata[\"relevance_spans\"] = []\n",
    "            \n",
    "            # Iterate through all matches in this page\n",
    "            for end_idx, (qa_idx, chunk_text) in automaton.iter(page_text):\n",
    "                start_idx = end_idx - len(chunk_text) + 1  # +1 because end_idx is inclusive\n",
    "                \n",
    "                span = {\n",
    "                    \"qa_id\": training_qas[qa_idx][\"id\"],\n",
    "                    \"page\": page_num,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx + 1  # Make end exclusive for easier indexing\n",
    "                }\n",
    "                page_doc.metadata[\"relevance_spans\"].append(span)\n",
    "                total_spans += 1\n",
    "        \n",
    "        logger.info(f\"Found {total_spans} relevance spans across all pages\")\n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Annotation failed: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to annotate documents: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d26b1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_text(docs: List[Document], chunk_size: int = 300, chunk_overlap: int = 30) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    \n",
    "    Why chunk?\n",
    "    - Embeddings work better on focused, semantic units\n",
    "    - Smaller chunks = more precise retrieval\n",
    "    - Overlap ensures we don't split important context\n",
    "    \n",
    "    Why these defaults?\n",
    "    - chunk_size=300: ~75 tokens, good for rule-specific content\n",
    "    - chunk_overlap=30: 10% overlap preserves context at boundaries\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects\n",
    "        chunk_size: Target size for each chunk (characters)\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk Documents with start_index in metadata\n",
    "        \n",
    "    Raises:\n",
    "        ChunkingError: If chunking process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chunk_size <= 0:\n",
    "            raise ChunkingError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "        \n",
    "        if chunk_overlap < 0:\n",
    "            raise ChunkingError(f\"chunk_overlap cannot be negative, got {chunk_overlap}\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ChunkingError(\n",
    "                f\"chunk_overlap ({chunk_overlap}) must be less than \"\n",
    "                f\"chunk_size ({chunk_size})\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Splitting documents with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,  # Use character count\n",
    "            add_start_index=True  # Critical: needed for coverage calculation\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ChunkingError):\n",
    "            raise\n",
    "        logger.error(f\"Chunking failed: {str(e)}\")\n",
    "        raise ChunkingError(f\"Failed to split documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COVERAGE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_overlap(span_start: int, span_end: int, chunk_start: int, chunk_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Compute character overlap between a relevance span and a chunk.\n",
    "    \n",
    "    Example:\n",
    "        Span:  [10, 30)  (relevant text from annotation)\n",
    "        Chunk: [20, 50)  (text chunk)\n",
    "        Overlap: [20, 30) = 10 characters\n",
    "    \n",
    "    Args:\n",
    "        span_start: Start index of relevance span\n",
    "        span_end: End index of relevance span (exclusive)\n",
    "        chunk_start: Start index of chunk\n",
    "        chunk_end: End index of chunk (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Number of overlapping characters\n",
    "    \"\"\"\n",
    "    overlap_start = max(span_start, chunk_start)\n",
    "    overlap_end = min(span_end, chunk_end)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "\n",
    "def generate_relevant_chunks_with_coverage(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate coverage scores for chunks containing ground truth spans.\n",
    "    \n",
    "    Coverage = (overlap_length / relevance_span_length)\n",
    "    \n",
    "    Why coverage?\n",
    "    - Measures \"how much of the relevant content is in this chunk\"\n",
    "    - Coverage=1.0: entire relevant span is in the chunk (perfect)\n",
    "    - Coverage=0.5: only half the relevant content is present\n",
    "    - Coverage=0.0: chunk doesn't contain relevant content\n",
    "    \n",
    "    This is better than binary relevance because:\n",
    "    - Distinguishes between partial and complete matches\n",
    "    - Handles cases where spans cross chunk boundaries\n",
    "    - Provides granular relevance scores for nDCG calculation\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of Documents containing only relevant chunks with coverage scores\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If coverage calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = chunk.metadata.get(\"start_index\", 0)\n",
    "            chunk_end = chunk_start + len(chunk.page_content)\n",
    "            relevance_spans = chunk.metadata.get(\"relevance_spans\", [])\n",
    "            \n",
    "            # Skip chunks without any ground truth annotations\n",
    "            if not relevance_spans:\n",
    "                continue\n",
    "            \n",
    "            # Create copy to avoid modifying original\n",
    "            annotated_chunk = copy.deepcopy(chunk)\n",
    "            annotated_chunk.metadata[\"coverage_per_query\"] = []\n",
    "            \n",
    "            for span in relevance_spans:\n",
    "                qa_id = span[\"qa_id\"]\n",
    "                \n",
    "                # Calculate how much of the span overlaps with this chunk\n",
    "                overlap_len = compute_overlap(\n",
    "                    span[\"start\"], span[\"end\"], \n",
    "                    chunk_start, chunk_end\n",
    "                )\n",
    "                \n",
    "                relevance_len = span[\"end\"] - span[\"start\"]\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if relevance_len == 0:\n",
    "                    logger.warning(f\"Zero-length relevance span for qa_id={qa_id}\")\n",
    "                    continue\n",
    "                \n",
    "                coverage = overlap_len / relevance_len\n",
    "                \n",
    "                # Skip queries with no overlap\n",
    "                if coverage == 0:\n",
    "                    continue\n",
    "                \n",
    "                annotated_chunk.metadata[\"coverage_per_query\"].append({\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"coverage\": coverage\n",
    "                })\n",
    "            \n",
    "            # Only keep chunks that have at least one relevant query\n",
    "            if annotated_chunk.metadata[\"coverage_per_query\"]:\n",
    "                annotated_chunk.metadata[\"chunk_id\"] = chunk_idx\n",
    "                relevant_chunks.append(annotated_chunk)\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_chunks)} relevant chunks out of {len(chunks)} total\")\n",
    "        return relevant_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Coverage calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate coverage: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_chunks_for_chroma(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter complex metadata for Chroma compatibility.\n",
    "    \n",
    "    Why needed?\n",
    "    - Chroma only supports simple types (str, int, float, bool)\n",
    "    - Complex types (lists, dicts) cause serialization errors\n",
    "    - We keep complex metadata in separate 'relevant_chunks' list\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Documents with filtered metadata safe for Chroma\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If metadata filtering fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrievable_docs = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Filter to simple metadata types\n",
    "            filtered_doc = filter_complex_metadata([chunk])[0]\n",
    "            \n",
    "            # Add chunk_id for later lookup\n",
    "            filtered_doc.metadata[\"chunk_id\"] = chunk_idx\n",
    "            \n",
    "            retrievable_docs.append(filtered_doc)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(retrievable_docs)} chunks for Chroma\")\n",
    "        return retrievable_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metadata filtering failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to prepare chunks: {str(e)}\") from e\n",
    "\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: List[Document], \n",
    "                   embedding_model: str = \"text-embedding-ada-002\", \n",
    "                   similarity_search: str = \"cosine\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist Chroma vector store.\n",
    "    \n",
    "    Note: This clears existing database!\n",
    "    - Ensures fresh embeddings\n",
    "    - Avoids stale data issues\n",
    "    - For production, consider incremental updates\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks (with simple metadata)\n",
    "        embedding_model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Initialized Chroma vector store\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If vector store creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an isolated temporary directory\n",
    "        tmp_dir = tempfile.mkdtemp(prefix=\"chroma_eval_\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Create the Chroma vector store from documents\n",
    "        db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=tmp_dir,\n",
    "            collection_metadata={\"hnsw:space\": similarity_search}\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Temporary Chroma DB created at: {tmp_dir}\")\n",
    "        return db, tmp_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Vector store creation failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to create vector store: {str(e)}\") from e\n",
    "\n",
    "def retrieve_top_k(\n",
    "    db: Chroma, \n",
    "    query: str, \n",
    "    k: int = 3\n",
    ") -> List[Tuple[str, str, int, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar chunks for a query.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (source, content, chunk_id, relevance_score)\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if k <= 0:\n",
    "            raise VectorStoreError(f\"k must be positive, got {k}\")\n",
    "        \n",
    "        logger.debug(f\"Retrieving top-{k} chunks for query: {query[:50]}...\")\n",
    "        \n",
    "        results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "        \n",
    "        formatted_results = [\n",
    "            (\n",
    "                doc.metadata.get(\"source\", \"unknown\"),\n",
    "                doc.page_content,\n",
    "                doc.metadata.get(\"chunk_id\", -1),\n",
    "                score\n",
    "            )\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to retrieve documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def dcg(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain.\n",
    "    \n",
    "    Formula: DCG = Σ(rel_i / log2(i + 2)) for i in range(len(scores))\n",
    "    \n",
    "    Why log2(i + 2)?\n",
    "    - Position 0: log2(2) = 1 (no discount)\n",
    "    - Position 1: log2(3) = 1.58 (small discount)\n",
    "    - Position 2: log2(4) = 2 (larger discount)\n",
    "    - Later positions are increasingly discounted\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (coverage values)\n",
    "        \n",
    "    Returns:\n",
    "        DCG score\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        dcg_value = np.sum([\n",
    "            rel / np.log2(idx + 2)\n",
    "            for idx, rel in enumerate(relevance_scores)\n",
    "        ])\n",
    "        \n",
    "        return float(dcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"DCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate DCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    nDCG = DCG / IDCG\n",
    "    \n",
    "    Why normalize?\n",
    "    - Makes scores comparable across queries\n",
    "    - Range: [0, 1] where 1 = perfect ranking\n",
    "    - Accounts for different numbers of relevant items\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores\n",
    "        \n",
    "    Returns:\n",
    "        nDCG score between 0 and 1\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG with actual ranking\n",
    "        dcg_value = dcg(relevance_scores)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg_value = dcg(ideal_scores)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg_value == 0:\n",
    "            logger.warning(\"IDCG is 0, returning nDCG=0\")\n",
    "            return 0.0\n",
    "        \n",
    "        ndcg_value = dcg_value / idcg_value\n",
    "        return float(ndcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"nDCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate nDCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def get_coverage(chunk_id: int, qa_id: str, relevant_chunks: List[Document]) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve coverage score for a specific chunk and query.\n",
    "    \n",
    "    This is a lookup function that connects:\n",
    "    - Retrieved chunk (by chunk_id from vector search)\n",
    "    - Query (by qa_id from evaluation set)\n",
    "    - Ground truth coverage (pre-computed in relevant_chunks)\n",
    "    \n",
    "    Args:\n",
    "        chunk_id: ID of the retrieved chunk\n",
    "        qa_id: ID of the query being evaluated\n",
    "        relevant_chunks: List of annotated chunks with coverage scores\n",
    "        \n",
    "    Returns:\n",
    "        Coverage score (0-1), or 0 if not found\n",
    "    \"\"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if chunk.metadata.get(\"chunk_id\") != chunk_id:\n",
    "            continue\n",
    "        \n",
    "        for coverage_entry in chunk.metadata.get(\"coverage_per_query\", []):\n",
    "            if coverage_entry[\"qa_id\"] == qa_id:\n",
    "                return coverage_entry[\"coverage\"]\n",
    "    \n",
    "    # Return 0 if chunk has no coverage for this query\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EVALUATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rag_system(pdf_path: str, training_qa_path: str,chunk_size: int = 300,chunk_overlap: int = 30,\n",
    "    k: int = 3, embedding_model: str = \"text-embedding-ada-002\", similarity_search: str = \"cosine\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete RAG evaluation pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load and clean PDF\n",
    "    2. Annotate with ground truth Q&A\n",
    "    3. Chunk documents\n",
    "    4. Calculate coverage for relevant chunks\n",
    "    5. Create vector store\n",
    "    6. For each query: retrieve top-k and calculate metrics\n",
    "    7. Report average DCG and nDCG\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to board game manual PDF\n",
    "        training_qa_path: Path to training Q&A JSON\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "        embedding_model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "        \n",
    "    Raises:\n",
    "        RAGEvaluationError: If any pipeline stage fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting RAG Evaluation Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load PDF\n",
    "        docs = load_documents(pdf_path)\n",
    "        \n",
    "        # Step 2: Annotate with ground truth\n",
    "        docs_with_qa = load_training_qa_to_docs(training_qa_path, docs)\n",
    "        \n",
    "        # Step 3: Chunk documents\n",
    "        chunks = split_text(docs_with_qa, chunk_size, chunk_overlap)\n",
    "\n",
    "        # Step 3.5: Save/Load Chunks\n",
    "        chunks_path = Path(pdf_path).with_name(\"CATAN_chunks.json\")\n",
    "        save_chunks(chunks, chunks_path)\n",
    "        # chunks = load_saved_chunks(chunks_path)\n",
    "        \n",
    "        # Step 4: Calculate coverage for relevant chunks\n",
    "        relevant_chunks = generate_relevant_chunks_with_coverage(chunks)\n",
    "        \n",
    "        # Step 5: Prepare and store in vector DB\n",
    "        chunks_for_chroma = prepare_chunks_for_chroma(chunks)\n",
    "        db, tmp_dir  = save_to_chroma(chunks_for_chroma, embedding_model, similarity_search)\n",
    "\n",
    "        # Step 6: Load evaluation queries\n",
    "        qa_data = load_json(training_qa_path)\n",
    "        evaluation_qas = qa_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not evaluation_qas:\n",
    "            raise EvaluationError(\"No evaluation queries found in JSON\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on {len(evaluation_qas)} queries\")\n",
    "        \n",
    "        # Step 7: Evaluate each query\n",
    "        dcg_values = []\n",
    "        ndcg_values = []\n",
    "        query_results = []\n",
    "        \n",
    "        for qa in evaluation_qas:\n",
    "            qa_id = qa.get(\"id\")\n",
    "            question = qa.get(\"question\")\n",
    "            gt_answer = qa.get(\"answer\")\n",
    "            \n",
    "            if not question:\n",
    "                logger.warning(f\"Skipping query with missing question: {qa_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Retrieve top-k chunks\n",
    "            top_k_results = retrieve_top_k(db, question, k=k)\n",
    "            top_k = []\n",
    "\n",
    "            # Calculate coverage scores for retrieved chunks\n",
    "            coverage_scores = []\n",
    "            for source, content, chunk_id, similarity_score in top_k_results:\n",
    "                coverage = get_coverage(chunk_id, qa_id, relevant_chunks)\n",
    "                coverage_scores.append(coverage)\n",
    "                top_k.append(content)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            query_dcg = dcg(coverage_scores)\n",
    "            query_ndcg = ndcg_at_k(coverage_scores)\n",
    "            \n",
    "            dcg_values.append(query_dcg)\n",
    "            ndcg_values.append(query_ndcg)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"qa_id\": qa_id,\n",
    "                \"question\": question,\n",
    "                \"top_k_content\": top_k,\n",
    "                \"gt_answer\": gt_answer,\n",
    "                \"coverage_scores\": coverage_scores,\n",
    "                \"dcg\": query_dcg,\n",
    "                \"ndcg\": query_ndcg\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_dcg = float(np.mean(dcg_values))\n",
    "        avg_ndcg = float(np.mean(ndcg_values))\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Average DCG:  {avg_dcg:.4f}\")\n",
    "        logger.info(f\"Average nDCG: {avg_ndcg:.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"avg_dcg\": avg_dcg,\n",
    "            \"avg_ndcg\": avg_ndcg,\n",
    "            \"num_queries\": len(evaluation_qas),\n",
    "            \"k\": k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"query_results\": query_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, RAGEvaluationError):\n",
    "            raise\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise RAGEvaluationError(f\"Evaluation pipeline failed: {str(e)}\") from e\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: List[str], isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6787e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:18:19,764 - INFO - Evaluating with parameters: chunk=300, overlap=30, top-k=3\n",
      "2025-11-14 23:18:19,766 - INFO - ============================================================\n",
      "2025-11-14 23:18:19,766 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-14 23:18:19,768 - INFO - ============================================================\n",
      "2025-11-14 23:18:19,769 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n",
      "2025-11-14 23:18:31,949 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-14 23:18:31,953 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_eval_small.json\n",
      "2025-11-14 23:18:31,954 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-14 23:18:31,955 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-14 23:18:31,957 - INFO - Found 11 relevance spans across all pages\n",
      "2025-11-14 23:18:31,958 - INFO - Splitting documents with chunk_size=300, overlap=30\n",
      "2025-11-14 23:18:31,973 - INFO - Created 100 chunks\n",
      "2025-11-14 23:18:31,981 - INFO - Found 15 relevant chunks out of 100 total\n",
      "2025-11-14 23:18:31,982 - INFO - Prepared 100 chunks for Chroma\n",
      "2025-11-14 23:18:33,454 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-14 23:18:34,687 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:35,166 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_eval_small.json\n",
      "2025-11-14 23:18:35,168 - INFO - Evaluating on 10 queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_cttyctr2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 23:18:35,717 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:36,121 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:36,128 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 23:18:36,497 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:36,736 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:36,741 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 23:18:37,153 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:37,620 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:37,848 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:38,169 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:38,724 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:38,729 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 23:18:38,943 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:38,947 - INFO - ============================================================\n",
      "2025-11-14 23:18:38,948 - INFO - Average DCG:  0.6197\n",
      "2025-11-14 23:18:38,949 - INFO - Average nDCG: 0.6182\n",
      "2025-11-14 23:18:38,949 - INFO - ============================================================\n",
      "2025-11-14 23:18:39,654 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:40,244 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:40,929 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:41,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:42,226 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:42,801 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:43,402 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:43,997 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:44,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:44,864 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]2025-11-14 23:18:48,710 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:49,482 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:18:50,496 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:51,726 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 1/50 [00:04<03:59,  4.89s/it]2025-11-14 23:18:52,768 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,772 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,775 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,778 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,781 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,785 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,791 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,794 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,800 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:52,816 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:18:58,530 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:18:59,838 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:00,885 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:16,616 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:19:17,710 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:18,812 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:21,366 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|▍         | 2/50 [00:33<15:04, 18.84s/it]2025-11-14 23:19:23,254 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:24,265 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▍        | 7/50 [00:37<03:01,  4.23s/it]2025-11-14 23:19:25,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:26,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,689 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,692 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,694 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,698 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,701 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,704 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,709 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,711 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:28,714 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:30,697 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:31,780 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▌        | 8/50 [01:06<06:10,  8.83s/it]2025-11-14 23:19:53,899 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:53,903 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:53,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 12/50 [01:10<03:02,  4.81s/it]2025-11-14 23:19:58,160 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:58,163 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 14/50 [01:11<02:13,  3.72s/it]2025-11-14 23:19:59,683 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:19:59,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:03,211 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 15/50 [01:18<02:29,  4.27s/it]2025-11-14 23:20:06,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,412 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,417 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,424 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,426 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,432 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,434 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:08,441 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|███▏      | 16/50 [01:22<02:19,  4.10s/it]2025-11-14 23:20:10,167 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:10,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:10,172 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:10,174 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:10,176 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:22,413 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:23,367 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▍      | 17/50 [01:42<04:09,  7.55s/it]2025-11-14 23:20:35,349 - INFO - Retrying request to /embeddings in 0.403242 seconds\n",
      "Evaluating:  42%|████▏     | 21/50 [01:47<01:58,  4.07s/it]2025-11-14 23:20:35,370 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|████▍     | 22/50 [01:48<01:39,  3.54s/it]2025-11-14 23:20:36,466 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▌     | 23/50 [01:49<01:23,  3.11s/it]2025-11-14 23:20:37,422 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,425 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,430 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,431 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,434 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,436 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,438 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,442 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,444 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,446 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,448 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:37,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:39,300 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:39,302 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:42,030 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:20:43,095 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:44,062 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:50,353 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:20:51,466 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:20:52,814 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 24/50 [02:16<03:30,  8.09s/it]2025-11-14 23:21:03,932 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:03,937 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:03,941 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  54%|█████▍    | 27/50 [02:22<02:01,  5.30s/it]2025-11-14 23:21:12,402 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:12,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 30/50 [02:27<01:15,  3.80s/it]2025-11-14 23:21:15,482 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:15,508 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▏   | 31/50 [02:28<01:03,  3.34s/it]2025-11-14 23:21:18,107 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,109 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,111 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,113 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,115 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,119 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,122 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,123 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,125 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:18,127 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:25,365 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 23:21:26,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:27,607 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:39,231 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:39,234 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:39,237 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:39,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|██████▍   | 32/50 [02:51<02:05,  7.00s/it]2025-11-14 23:21:39,253 - INFO - Retrying request to /chat/completions in 0.416258 seconds\n",
      "Evaluating:  74%|███████▍  | 37/50 [02:56<00:46,  3.61s/it]2025-11-14 23:21:44,624 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:44,626 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:46,249 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:46,252 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:53,995 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:53,998 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:54,001 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:54,006 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:54,008 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:54,011 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:21:54,013 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:01,940 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:03,048 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|███████▊  | 39/50 [03:19<01:00,  5.51s/it]2025-11-14 23:22:07,270 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:07,273 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:07,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:07,277 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|████████▌ | 43/50 [03:21<00:23,  3.42s/it]2025-11-14 23:22:10,867 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:10,875 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 44/50 [03:25<00:21,  3.58s/it]2025-11-14 23:22:13,775 - INFO - Retrying request to /chat/completions in 0.391874 seconds\n",
      "2025-11-14 23:22:13,776 - INFO - Retrying request to /chat/completions in 0.434818 seconds\n",
      "2025-11-14 23:22:16,095 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:17,697 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:18,993 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:18,996 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:19,000 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 46/50 [03:32<00:13,  3.49s/it]2025-11-14 23:22:20,259 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▍| 47/50 [03:36<00:10,  3.60s/it]2025-11-14 23:22:24,436 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|█████████▊| 49/50 [03:39<00:02,  2.98s/it]2025-11-14 23:22:27,933 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 23:22:29,101 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 50/50 [03:42<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "📁 Generation results saved to rag_retrieval_eval.csv\n",
      "📁 Generation results saved to rag_generation_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PDF_PATH = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "    TRAINING_QA_PATH = \"data/BoardGamesRuleBook/CATAN_eval_small.json\"\n",
    "    CHUNK_SIZES = [300]\n",
    "    CHUNK_OVERLAPS = [30]\n",
    "    SIMILARITY_SEARCH = \"cosine\"\n",
    "    Ks = [3]\n",
    "    \n",
    "    retrieval_eval_results = []\n",
    "    generation_eval_results = []\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "    \n",
    "    for CHUNK_SIZE, CHUNK_OVERLAP, k in product(CHUNK_SIZES, CHUNK_OVERLAPS, Ks):\n",
    "        logger.info(f\"Evaluating with parameters: chunk={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top-k={k}\")\n",
    "        try:\n",
    "            # Retrieval evaluation\n",
    "            results = evaluate_rag_system(\n",
    "                pdf_path=PDF_PATH,\n",
    "                training_qa_path=TRAINING_QA_PATH,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                similarity_search=SIMILARITY_SEARCH,\n",
    "                k=k\n",
    "            )\n",
    "\n",
    "            retrieval_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZE,\n",
    "                \"overlap\": CHUNK_OVERLAP,\n",
    "                \"top_k\": k,\n",
    "                **results,\n",
    "            })\n",
    "\n",
    "            # Generation evaluation\n",
    "            evaluation_rows = []\n",
    "            for query_result in results.get(\"query_results\"):\n",
    "                question = query_result.get(\"question\")\n",
    "                top_k_content = query_result.get(\"top_k_content\")\n",
    "                gt_answer = query_result.get(\"gt_answer\")\n",
    "\n",
    "                answer = generate_answer(question, top_k_content)\n",
    "                evaluation_rows.append({\n",
    "                            \"question\": question,\n",
    "                            \"contexts\": top_k_content,\n",
    "                            \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "                            \"reference\": gt_answer,\n",
    "                        })\n",
    "            ragas_eval_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "            # Run evaluation\n",
    "            scores = evaluate(\n",
    "                ragas_eval_dataset,\n",
    "                metrics=[\n",
    "                    answer_correctness,\n",
    "                    answer_relevancy,\n",
    "                    faithfulness,\n",
    "                    context_precision,\n",
    "                    context_recall,\n",
    "                ],\n",
    "                llm=llm,  # pass the LLM explicitly\n",
    "            )\n",
    "\n",
    "            df_score = scores.to_pandas()\n",
    "\n",
    "\n",
    "            generation_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZES,\n",
    "                \"chunk_overlap\": CHUNK_OVERLAPS,\n",
    "                \"embedding_model\": [\"text-embedding-3-small\"],\n",
    "                \"top_k\": Ks,\n",
    "                \"answer_correctness\": np.mean(df_score[\"answer_correctness\"]),\n",
    "                \"answer_relevancy\": np.mean(df_score[\"answer_relevancy\"]),\n",
    "                \"faithfulness\": np.mean(df_score[\"faithfulness\"]),\n",
    "                \"context_precision\": np.mean(df_score[\"context_precision\"]),\n",
    "                \"context_recall\": np.mean(df_score[\"context_recall\"]),\n",
    "            })\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "        except RAGEvaluationError as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Convert to DataFrame for easy comparison, save the .csv\n",
    "    df = pd.DataFrame(retrieval_eval_results)\n",
    "    df.to_csv(\"rag_retrieval_eval.csv\", index=False)\n",
    "    print(\"📁 Generation results saved to rag_retrieval_eval.csv\")\n",
    "    df = pd.DataFrame(generation_eval_results)\n",
    "    df.to_csv(\"rag_generation_eval.csv\", index=False)\n",
    "    print(\"📁 Generation results saved to rag_generation_eval.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2623475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[300]</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[text-embedding-3-small]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>0.492861</td>\n",
       "      <td>0.962708</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_size chunk_overlap           embedding_model top_k  \\\n",
       "0      [300]          [30]  [text-embedding-3-small]   [3]   \n",
       "\n",
       "   answer_correctness  answer_relevancy  faithfulness  context_precision  \\\n",
       "0            0.492861          0.962708      0.458333           0.733333   \n",
       "\n",
       "   context_recall  \n",
       "0             0.9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verification.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
