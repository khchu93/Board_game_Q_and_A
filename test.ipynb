{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9741f3",
   "metadata": {},
   "source": [
    "### Building a Local Knowledge Assistant with LangChain & OpenAI\n",
    "\n",
    "In this project, I implemented a `Retrieval-Augmented Generation (RAG)` pipeline using `LangChain`, `Chroma`, and `OpenAI embeddings`. The system retrieves relevant chunks of information from my document collection to provide more accurate and context-aware answers. By combining retrieval with generation, I can leverage a smaller, pre-trained LLM while still achieving detailed and precise responses, without the need for costly fine-tuning. This setup allows me to experiment with AI-driven Q&A in a practical, hands-on way.\n",
    "\n",
    "This project was inspired by [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch?v=tcqEUSNCn8I) and adapted for personal learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf581640",
   "metadata": {},
   "source": [
    "### Why RAG Matters?\n",
    "\n",
    "As language models grow larger, updating and controlling their knowledge becomes increasingly impractical through fine-tuning alone. `RAG` introduces a retrieval-based paradigm that preserves model generality while enabling precise, real-time knowledge integration. &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8965fb",
   "metadata": {},
   "source": [
    "**Traditional Approaches** <br>\n",
    "**`Fine-tuning`** works but comes with serious baggage: massive **compute requirements**, maintaining multiple **model versions**, knowledge that goes **stale** (requiring full retraining), and catastrophic **forgetting** where models lose general capabilities. **`LoRA`** makes this more efficient by training **only low-rank matrices** instead of full weights, but you're still **burning GPU hours**, tuning **hyperparameters**, and—critically—**baking knowledge** into opaque model weights. Want to update a single fact? Good luck auditing which training examples influenced what, or making changes without retraining from scratch.\n",
    "\n",
    "**RAG: Knowledge Without Retraining** <br>\n",
    "`RAG` flips the script. Instead of modifying the model, it retrieves **relevant context** from **vector databases at inference time**. The advantages are game-changing: **instant knowledge updates** without retraining, **traceable citations** grounding outputs in source material, and **zero training compute**—just embed and index your documents. Your knowledge stays **external** and **auditable**, perfect for rapidly changing domains like customer support or compliance where information updates weekly. Plus, you get clean separation of concerns: **the LLM handles reasoning while retrieval manages knowledge**.\n",
    "\n",
    "**Trade-offs** <br>\n",
    "The catch? You're now in the **retrieval** game. Chunking strategies, embedding quality, and similarity metrics directly impact what context reaches your model. Poor retrieval means **hallucinations**. **Context windows** limit how much you can include, requiring **smart ranking** when relevant info spans many documents. But these challenges beat managing expensive retraining pipelines—RAG trades GPU clusters for vector database tuning, a much more tractable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47d63f",
   "metadata": {},
   "source": [
    "### Why LangChain?\n",
    "\n",
    "LLM applications in production need to integrate with dozens of **services—vector databases**, **document loaders**, **model providers**, **monitoring tools**. Alternatives like `Mirascope` or `LiteLLM` focus on **specific integration** challenges, but `LangChain`'s ecosystem is unmatched: **100+ integrations** maintained by both the core team and community, reducing custom boilerplate significantly.  &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225221e2",
   "metadata": {},
   "source": [
    "**Going Solo: Reinventing the Wheel** <br>\n",
    "**Going solo means building infrastructure from scratch**. `Raw API` calls seem simple until you're managing conversation history, implementing retry logic, handling streaming responses, and coordinating multi-step chains. **Custom frameworks give you control but demand serious engineering**: designing abstractions, integrating new model providers, building observability, maintaining compatibility. Every `RAG` pipeline or `agent system` requires **solving the same orchestration problems** teams everywhere have already tackled.\n",
    "\n",
    "**What LangChain Brings to the Table** <br>\n",
    "`LangChain` consolidates years of collective learning into **reusable patterns**. The framework shines in three areas: **provider abstraction** (switch between OpenAI, Anthropic, local models without rewriting logic), **composable chains** (LCEL makes complex workflows readable and debuggable), and **ecosystem integrations** (vector stores, document loaders, agent tools—all plug-and-play). You're not just getting code—you're getting architectural patterns that scale, from simple prompt chains to sophisticated agent loops with memory and tool use.\n",
    "\n",
    "**What You're Trading For: Velocity vs Transparency** <br>\n",
    "The tradeoff is **framework dependency**. `LangChain` adds abstraction layers that can **obscure what's happening under the hood**. Simple tasks might feel overengineered, and rapid framework evolution means staying current with breaking changes. Debugging requires understanding LangChain's execution model, not just your application logic. But the math usually works out: teams shipping complex LLM features fast versus teams stuck building plumbing. For most production use cases, LangChain's velocity wins outweigh the framework overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef501fb",
   "metadata": {},
   "source": [
    "### Why Chroma?\n",
    "\n",
    "When working on LLM projects that involve `semantic search` or `RAG`, vector databases are essential for storing and querying embeddings efficiently. `Chroma` stands out because it **balances ease-of-use, flexibility, and functionality**. Unlike `FAISS` (a low-level similarity search library), `Pinecone` (managed cloud service), or `Weaviate` (complex self-hosted solution), `Chroma` lets you run embedded in **your application** or as a **standalone server**, supports metadata and persistence out-of-the-box, and integrates seamlessly with frameworks like LangChain—making it ideal for developers who want to move fast without infrastructure overhead. &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b1427",
   "metadata": {},
   "source": [
    "**The Vector Database Landscape** <br>\n",
    "Many other solutions have their trade-offs. `FAISS` is extremely fast at large-scale similarity search but requires **manual management of metadata and persistence**. `Pinecone` offers a fully managed solution with advanced features but comes with **ongoing costs** and **cloud-only** limitations. `Weaviate` provides hybrid search and ML model integration but has a **steeper learning curve** and more **complex setup**. For engineers, these hurdles can slow down project development.\n",
    "\n",
    "**Chroma: Simplicity Without Sacrifice** <br>\n",
    "`Chroma`, on the other hand, simplifies the workflow. Its Python API is intuitive, it integrates naturally with `LangChain`, and it supports both **embedded** and **client-server deployment** modes. You can store embeddings along with metadata, filter results easily, and get your RAG pipelines running quickly. For small-to-medium scale projects, it allows you to focus on building intelligent LLM applications rather than spending time on database plumbing.\n",
    "\n",
    "**Where Chroma Makes Trade-offs** <br>\n",
    "Of course, `Chroma` has limitations. It **isn't optimized for extremely large datasets** with hundreds of millions of vectors and lacks some of the **advanced indexing options** that `FAISS` offers. However, for engineers who want to rapidly prototype and deploy LLM-powered solutions, `Chroma` provides a perfect balance of simplicity, power, and flexibility to get real results without unnecessary friction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Adapted from \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "# https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "import openai \n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca6c1",
   "metadata": {},
   "source": [
    "The project begins by **converting the target documents into LangChain** `Document` **objects** using the `DirectoryLoader` function. This preserves both the **text** and **metadata**, such as the source path and start index, which are required for LangChain's downstream functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw files into Document\n",
    "def load_documents(DATA_PATH: str):\n",
    "    # loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")  # Alice in the wonderland\n",
    "    # documents = loader.load()\n",
    "    documents = []\n",
    "    for pdf_file in glob.glob(f\"{DATA_PATH}/*.pdf\"):\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e062cf",
   "metadata": {},
   "source": [
    "Next, the documents are **split into smaller chunks**. This is necessary primarily for **retrieval precision in RAG systems**. Smaller chunks allow the system to retrieve **only the most relevant information** rather than entire documents, improving semantic similarity matching. **Large chunks** can **dilute semantic meaning** and introduce **irrelevant context**, while **smaller chunks** enable more **focused retrieval**. Additionally, chunking helps manage **LLM context window limitations**, though retrieval quality is the primary consideration.\n",
    "\n",
    "The 'RecursiveCharacterTextSplitter' controls how text is divided using two key parameters: 'chunk_size' and 'chunk_overlap'. 'chunk_size' defines the *maximum length* of each chunk, while 'chunk_overlap' *repeats a portion* of the previous chunk in the next one to **reduce the chance of splitting related information** across chunk boundaries. Choosing appropriate values for these parameters directly impacts retrieval quality, and thus the overall performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0be34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into small chunks\n",
    "def split_text(documents: list[Document], chunk_size: int=300, chunk_overlap: int=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,          #nb of characters in each chunk\n",
    "        chunk_overlap=chunk_overlap,    #nb of characters to overlap between chunks\n",
    "        length_function=len,            #decide how to measure the chunk, e.g., character, token, etc\n",
    "        add_start_index=True,           #add the starting index of the chunk\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} pages into {len(chunks)} chunks.\\n\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d6bb",
   "metadata": {},
   "source": [
    "Once the documents are split, each chunk is converted into a **vector embedding** using OpenAI's 'text-embedding-3-small' model. These **embeddings** are then stored in a 'Chroma' vector database along with the **original chunk text and metadata**. The 'Chroma' database enables efficient **similarity search** by **comparing query embeddings against stored chunk embeddings**.\n",
    "\n",
    "It is crucial to use the **same embedding model** for both **indexing chunks** and **embedding queries** because different models produce **incompatible vector representations** that exist in different semantic spaces, making retrieval unreliable or impossible. For this project, I use the default embedding model `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vector embedding to chunks and save the embedding vector along with the content and metadata to database\n",
    "def save_to_chroma(chunks: list[Document], CHROMA_PATH: str, model=\"text-embedding-ada-002\"):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(model=model), persist_directory=None\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c29e2",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b93f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 18 documents into 328 chunks.\n",
      "\n",
      "Print chunk 10 content: \n",
      "Content: \"returns a generated text completion. These input\n",
      "contexts can contain thousands of tokens, espe-\n",
      "cially when language models are used to process\n",
      "long documents (e.g., legal or scientific documents,\n",
      "conversation histories, etc.) or when language mod-\"\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-11-22T01:09:40+00:00', 'author': '', 'keywords': '', 'moddate': '2023-11-22T01:09:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/papers\\\\Lost in the Middle_ How Language Models Use Long Contexts.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'start_index': 1743}\n",
      "\n",
      "Saved 328 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "# DATA_PATH = \"data/books\"    # Alice in the wonderland\n",
    "DATA_PATH = \"data/papers\"    # LLMs paper\n",
    "\n",
    "# split the document in chunks and save it to database along with its embedded vector\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_text(documents)\n",
    "\n",
    "print(f\"Print chunk 10 content: \")\n",
    "document = chunks[10]\n",
    "print(f\"Content: \\\"{document.page_content}\\\"\")\n",
    "print(f\"Metadata: {document.metadata}\\n\")\n",
    "\n",
    "db = save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac1a",
   "metadata": {},
   "source": [
    "The **prompt template** determines how **retrieved context** and the **user query** are **structured** for the **language model**. While prompt quality is difficult to quantify precisely, adhering to **established prompting principles significantly improves model responses**. Effective LLM prompting requires **clear, specific instructions** - similar to providing detailed directions to a new team member. Well-defined prompts guide the model toward desired outputs, while vague or ambiguous instructions increase output unpredictability, often resulting in irrelevant or inaccurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ed89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d87c",
   "metadata": {},
   "source": [
    "Once the Chroma database and prompt template are configured, the system can process user queries through the following pipeline: First, the query is embedded using the **same embedding model** applied to the document chunks, ensuring vector space consistency. The system then retrieves the **top-k most similar chunks** from the vector database (using L2 (Euclidean) distance by default). These retrieved chunks are combined with the original query according to the prompt template structure and passed to the LLM (default LLM: `gpt-3.5-turbo`) for response generation.\n",
    "\n",
    "Implementing **quality safeguards** is essential for production systems. This includes **rejecting empty** or **malformed queries** and filtering results when similarity scores fall **below a confidence threshold**, as low-similarity retrievals typically indicate insufficient relevant context and lead to unreliable outputs. With these components in place, the RAG pipeline can effectively retrieve pertinent information and generate well-informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bfcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: str, isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in context])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17466451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results, similarity too low.\n",
      "Top 2 similarity:\n",
      "Top 1:\n",
      "Content: even for explicitly long-context models. Our\n",
      "analysis provides a better understanding of\n",
      "how language models use their input context\n",
      "and provides new evaluation protocols for\n",
      "future long-context language models.\n",
      "1 Introduction\n",
      "Language models have become an important and\n",
      "L2 similarity: 0.6654352187271577\n",
      "\n",
      "Top 2:\n",
      "Content: shows that precise knowledge access on long con-\n",
      "texts may be an added challenge.\n",
      "6.2 How Do Language Models Use Context?\n",
      "The pioneering work of Khandelwal et al. (2018)\n",
      "showed that small LSTM language models make\n",
      "increasingly coarse use of longer-term context;\n",
      "L2 similarity: 0.6440712551715008\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "even for explicitly long-context models. Our\n",
      "analysis provides a better understanding of\n",
      "how language models use their input context\n",
      "and provides new evaluation protocols for\n",
      "future long-context language models.\n",
      "1 Introduction\n",
      "Language models have become an important and\n",
      "\n",
      "---\n",
      "\n",
      "shows that precise knowledge access on long con-\n",
      "texts may be an added challenge.\n",
      "6.2 How Do Language Models Use Context?\n",
      "The pioneering work of Khandelwal et al. (2018)\n",
      "showed that small LSTM language models make\n",
      "increasingly coarse use of longer-term context;\n",
      "\n",
      "---\n",
      "\n",
      "Our analysis provides a better understanding of\n",
      "how language models use their input context and\n",
      "introduces new evaluation protocols for future long-\n",
      "context models; to claim that a language model can\n",
      "robustly use information within long input con-\n",
      "texts, it is necessary to show that its performance\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How does the length of a context affect a language model’s ability to use information?\n",
      "\n",
      "Response: The length of a context can affect a language model's ability to use information by potentially making it more challenging for the model to access precise knowledge on longer contexts. Additionally, longer contexts may result in language models making increasingly coarse use of longer-term context.\n",
      "Sources: ['data/papers\\\\Lost in the Middle_ How Language Models Use Long Contexts.pdf', 'data/papers\\\\Lost in the Middle_ How Language Models Use Long Contexts.pdf', 'data/papers\\\\Lost in the Middle_ How Language Models Use Long Contexts.pdf']\n"
     ]
    }
   ],
   "source": [
    "# query_text = input(\"Enter your query: \")\n",
    "query_text = \"How does the length of a context affect a language model’s ability to use information?\"\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results, similarity too low.\")\n",
    "    # return\n",
    "\n",
    "print(\"Top 2 similarity:\")\n",
    "for i, k in enumerate(results):\n",
    "    if i == 2:\n",
    "        break\n",
    "    print(f\"Top {i + 1}:\\nContent: {k[0].page_content}\\nL2 similarity: {k[1]}\\n\")\n",
    "\n",
    "response_text = generate_answer(query_text, results, True)\n",
    "\n",
    "# Print the formatted response\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text.content}\\nSources: {sources}\"\n",
    "print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722ea44",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "`RAG` evaluation breaks down into two critical stages: **retrieval** and **generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d92424",
   "metadata": {},
   "source": [
    "**Retrieval Evaluation**<br>\n",
    "In retrieval evaluation, we **measure how effectively the system identifies and ranks relevant documents for a given query**. Standard metrics like `Precision@k`, `Recall@k`, and `Mean Reciprocal Rank (MRR)` quantify whether the retriever surfaces useful chunks in its top results. High precision means most retrieved chunks are relevant; high recall means we're not missing critical information. These metrics guide optimization of chunk size, overlap, and embedding model selection—essentially tuning what context reaches the generator.\n",
    "\n",
    "**Generation Evaluation**<br>\n",
    "Generation evaluation **assesses the quality and factual grounding of responses given the retrieved context**. We use `RAGAS (Retrieval-Augmented Generation Assessment)` metrics: `faithfulness` (does the answer align with retrieved evidence?), answer correctness (how close to ground truth?), `answer relevancy` (is it on-topic?), and `context precision/recall` (did we retrieve the right supporting passages?). These metrics reveal not just what the model generates, but how well it uses retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fbc06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content from Q&A file generated with ChatGPT\n",
    "def read_qa_file_by_paper(file_path: str):\n",
    "    rows = []\n",
    "    current_paper = None\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "            if line.lower().startswith(\"paper:\"):\n",
    "                current_paper = line[len(\"paper:\"):].strip()\n",
    "            elif line.lower().startswith(\"q:\"):\n",
    "                question = line[len(\"q:\"):].strip()\n",
    "            elif line.lower().startswith(\"a:\"):\n",
    "                answer = line[len(\"a:\"):].strip()\n",
    "                if current_paper is None:\n",
    "                    raise ValueError(f\"Answer found without specifying paper: {line}\")\n",
    "                rows.append({\n",
    "                    \"paper\": current_paper,\n",
    "                    \"question\": question,\n",
    "                    \"reference\": answer\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(f\"Line format not recognized: {line}\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91211a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_in_eval_format(path: str, chroma: Chroma, k: int):\n",
    "    evaluation_rows = []\n",
    "\n",
    "    # Extract Q&A content from txt\n",
    "    qa_rows = read_qa_file_by_paper(path)\n",
    "\n",
    "    for row in qa_rows:\n",
    "        question = row[\"question\"]\n",
    "        ground_truth = row[\"reference\"]\n",
    "        paper = row[\"paper\"]\n",
    "\n",
    "        # Retrieve context from database and generate the answer\n",
    "        context = chroma.similarity_search_with_relevance_scores(question, k=k)\n",
    "        answer = generate_answer(question, context)\n",
    "\n",
    "        # Organize the information into the format required by RAGAS\n",
    "        evaluation_rows.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": [doc.page_content for doc, _score in context],  # <-- plain text\n",
    "            # \"answer\": answer_with_source,\n",
    "            \"answer\": answer.content,\n",
    "            \"reference\": ground_truth,\n",
    "            \"source\": paper\n",
    "        })\n",
    "    return evaluation_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "question answering example presented in Figure 2. Re-\n",
      "ordering the documents in the input context does not\n",
      "affect the desired output.\n",
      "Our experimental setup is similar to the needle-\n",
      "in-a-haystack experiments of Ivgi et al. (2023), who\n",
      "compare question answering performance when the\n",
      "\n",
      "---\n",
      "\n",
      "models must retrieve from the middle of the input context.\n",
      "placed at the start of the input context, LongChat-\n",
      "13B (16K) tends to generate code to retrieve the\n",
      "key, rather than outputting the value directly.\n",
      "4 Why Are Language Models Not Robust\n",
      "to Changes in the Position of Relevant\n",
      "Information?\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: When I retrieve 5 chunks for my RAG system, does it matter what order I put them in?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "of the context, and slightly increases performance\n",
      "when using information in the middle and end of\n",
      "the context.\n",
      "D GPT-4 Performance\n",
      "We evaluate GPT-4 (8K) on a subset of 500 ran-\n",
      "dom multi-document QA examples with 20 total\n",
      "documents in each input context (Figure 15). GPT-\n",
      "\n",
      "---\n",
      "\n",
      "context, not all models achieve high performance.\n",
      "Similar to our multi-document QA results, GPT-\n",
      "3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\n",
      "Instruct have the lowest performance when they\n",
      "must access key-value pairs in the middle of their\n",
      "input context. LongChat-13B (16K) exhibits a dif-\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What about longer contexts? If I use GPT-4 with 128k context window, can I just dump all my documents in there?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "bias (10-point worst-case degradation), but the bias\n",
      "remains significant. However, the 70B models\n",
      "with and without additional fine-tuning have largely\n",
      "similar trends (showing both primacy and recency\n",
      "bias), and additional fine-tuning minimally changes\n",
      "the positional bias severity.\n",
      "\n",
      "---\n",
      "\n",
      "a dramatic primacy and recency bias—there is a\n",
      "20-point accuracy disparity between the best- and\n",
      "worst-case performance. Applying additional fine-\n",
      "tuning to the 13B seems to slightly reduce this\n",
      "bias (10-point worst-case degradation), but the bias\n",
      "remains significant. However, the 70B models\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How much worse is the middle really? Is it noticeable?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "claude-1.3\n",
      "claude-1.3-100k\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-16k-0613\n",
      "mpt-30b-instruct\n",
      "longchat-13b-16k\n",
      "gpt-4-0613\n",
      "Figure 15: Although GPT-4 has higher absolute perfor-\n",
      "mance than other models, its performance still degrades\n",
      "when relevant information occurs in the middle of the\n",
      "input context.\n",
      "\n",
      "---\n",
      "\n",
      "context, not all models achieve high performance.\n",
      "Similar to our multi-document QA results, GPT-\n",
      "3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\n",
      "Instruct have the lowest performance when they\n",
      "must access key-value pairs in the middle of their\n",
      "input context. LongChat-13B (16K) exhibits a dif-\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Does this happen with all models or just GPT?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "is a paragraph (as opposed to a list or a table). We\n",
      "use passages (chunks of at most 100 tokens) from\n",
      "Wikipedia as documents within our input contexts.\n",
      "For each of the queries, we need a document\n",
      "that contains the answer and k − 1 distractor\n",
      "documents that do not contain the answer. To\n",
      "\n",
      "---\n",
      "\n",
      "models must retrieve from the middle of the input context.\n",
      "placed at the start of the input context, LongChat-\n",
      "13B (16K) tends to generate code to retrieve the\n",
      "key, rather than outputting the value directly.\n",
      "4 Why Are Language Models Not Robust\n",
      "to Changes in the Position of Relevant\n",
      "Information?\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What if I have 10 chunks? Where should I put them?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "models must retrieve from the middle of the input context.\n",
      "placed at the start of the input context, LongChat-\n",
      "13B (16K) tends to generate code to retrieve the\n",
      "key, rather than outputting the value directly.\n",
      "4 Why Are Language Models Not Robust\n",
      "to Changes in the Position of Relevant\n",
      "Information?\n",
      "\n",
      "---\n",
      "\n",
      "these models are evaluated on sequences longer than those seen during training (center and right subplots), we\n",
      "observe a U-shaped performance curve—performance is higher when relevant information occurs at the beginning\n",
      "or end of the input context, as opposed to the middle of the input context.\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Can I just tell the model \"pay attention to the middle\"?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "robustly use information within long input con-\n",
      "texts, it is necessary to show that its performance\n",
      "is minimally affected by the position of the rele-\n",
      "vant information in the input context (e.g., minimal\n",
      "difference in best- and worst-case performance).\n",
      "\n",
      "---\n",
      "\n",
      "the beginning of the input context) when prompted\n",
      "with instruction-formatted data. We hypothesize\n",
      "that non-instruction fine-tuned language models\n",
      "learn to use these long contexts from similarly-\n",
      "formatted data that may occur in Internet text seen\n",
      "during pre-training, e.g., StackOverflow questions\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What's the minimum context length where this matters?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "supervised fine-tuning on a dataset of instructions\n",
      "and responses. The task specification and/or in-\n",
      "struction is commonly placed at the beginning of\n",
      "the input context in supervised instruction fine-\n",
      "tuning data, which might lead instruction fine-\n",
      "tuned language models to place more weight on\n",
      "\n",
      "---\n",
      "\n",
      "input contexts. Comparing the results in §2.3 with\n",
      "those when randomizing the distractor order and\n",
      "mentioning such in the prompt, we see that ran-\n",
      "domization slightly decreases performance when\n",
      "the relevant information is at the very beginning\n",
      "of the context, and slightly increases performance\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Does this apply to my prompt instructions too?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "document question answering performance. Lower positions are closer to the start of the input context. Performance\n",
      "is highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models\n",
      "must reason over information in the middle of their input context.\n",
      "\n",
      "---\n",
      "\n",
      "single document that contains the answer and must\n",
      "use it to answer the question.\n",
      "Model performance is highest when relevant in-\n",
      "formation occurs at the beginning or end of its\n",
      "input context. As illustrated in Figure 5, chang-\n",
      "ing the position of relevant information in the in-\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What about question-answering? Should I put the question at the start or end?\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70Accuracy\n",
      "20 T otal Retrieved Documents (~4K tokens)\n",
      "1st 5th 10th 15th 20th 25th 30th\n",
      "Position of Document with the Answer\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70Accuracy\n",
      "30 T otal Retrieved Documents (~6K tokens)\n",
      "mpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2\n",
      "\n",
      "---\n",
      "\n",
      "10 docs 20 docs 30 docs\n",
      "avg ± stdev max avg ± stdev max avg ± stdev max\n",
      "LongChat-13B (16K) 1749.9 ± 112.4 2511 3464.6 ± 202.3 4955 5181.9 ± 294.7 7729\n",
      "MPT-30B 1499.7 ± 88.5 1907 2962.4 ± 158.4 3730 4426.9 ± 230.5 5475\n",
      "GPT-3.5-Turbo 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Is there a sweet spot for number of chunks?\n",
      "\n",
      "Dataset({\n",
      "    features: ['question', 'contexts', 'answer', 'reference', 'source'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# generate answer in evaluation format\n",
    "qa_file = \"data/papers/Beginner Q&A.txt\"\n",
    "evaluation_rows = generate_answer_in_eval_format(path=qa_file, chroma=db, k=2)\n",
    "\n",
    "# change it to Dataset format\n",
    "evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "print(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49982d1e",
   "metadata": {},
   "source": [
    "**Why RAGAS?** <br>\n",
    "Traditional NLP metrics like `BLEU` or `ROUGE` measure surface-level text similarity but ignore whether answers are grounded in evidence—critical for `RAG` systems where factual accuracy and traceability matter. `RAGAS` provides task-aware, automated evaluation that directly **measures how generated content connects to retrieved context**. Its modular design integrates seamlessly with `LangChain` and `Weights & Biases`, enabling scalable experimentation and real-time tracking. Essentially, `RAGAS` lets us **evaluate retrieval-generation alignment** systematically, leading to more interpretable and reliable `RAG` performance insights without constant human annotation.\n",
    "\n",
    "**The limitations?** <br>\n",
    "`RAGAS` relies on **LLM-as-judge evaluation**, meaning it **uses language models (often GPT-4) to score metrics** like `faithfulness` and `relevancy`. This introduces **cost** (API calls for every evaluation), **latency** (slower than traditional metrics), and **potential bias** (the judge model's own limitations affect scores). Additionally, `RAGAS` metrics **require ground truth datasets** for answer correctness, which aren't always available or easy to create for domain-specific applications. The framework also has a **learning curve**—understanding what each metric measures and how to interpret scores together requires familiarity with `RAG`-specific evaluation concepts. Despite these trade-offs, for most `RAG` applications, `RAGAS`'s ability to measure grounding and factuality outweighs the operational overhead compared to purely manual evaluation or metrics that ignore retrieval quality entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd22998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  16%|█▌        | 8/50 [00:31<01:48,  2.58s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  42%|████▏     | 21/50 [01:10<01:15,  2.61s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  54%|█████▍    | 27/50 [01:33<01:06,  2.89s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  66%|██████▌   | 33/50 [01:40<00:32,  1.94s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 50/50 [02:29<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset preview:\n",
      "1. Q: When I retrieve 5 chunks for my RAG system, does it matter what order I put them in?\n",
      "   Answer: No, re-ordering the documents in the input context does not affect the desired output in the given scenario.\n",
      "   Reference: YES! The paper shows LLMs perform ~20% worse on information in the middle of context vs. beginning/end. For RAG, put your most relevant chunks at the START and END of your prompt. If you have 5 chunks ranked by relevance: [1st, 5th, 4th, 3rd, 2nd] - sandwich low-importance chunks in the middle.\n",
      "   Contexts: ['question answering example presented in Figure 2. Re-\\nordering the documents in the input context does not\\naffect the desired output.\\nOur experimental setup is similar to the needle-\\nin-a-haystack experiments of Ivgi et al. (2023), who\\ncompare question answering performance when the', 'models must retrieve from the middle of the input context.\\nplaced at the start of the input context, LongChat-\\n13B (16K) tends to generate code to retrieve the\\nkey, rather than outputting the value directly.\\n4 Why Are Language Models Not Robust\\nto Changes in the Position of Relevant\\nInformation?']\n",
      "\n",
      "2. Q: What about longer contexts? If I use GPT-4 with 128k context window, can I just dump all my documents in there?\n",
      "   Answer: Based on the context provided, longer contexts may not necessarily lead to improved performance with GPT-4. It is mentioned that not all models achieve high performance when accessing key-value pairs in the middle of their input context. Therefore, simply dumping all your documents into a 128K context window may not necessarily result in better performance, as the model may still struggle with accessing information in the middle of the context.\n",
      "   Reference: No! Even with huge context windows, models still struggle with middle content. The paper tested up to 32k tokens and found the U-shaped pattern persists. More context ≠ better performance. Strategy: Be selective about what you include and carefully position critical information at edges.\n",
      "   Contexts: ['of the context, and slightly increases performance\\nwhen using information in the middle and end of\\nthe context.\\nD GPT-4 Performance\\nWe evaluate GPT-4 (8K) on a subset of 500 ran-\\ndom multi-document QA examples with 20 total\\ndocuments in each input context (Figure 15). GPT-', 'context, not all models achieve high performance.\\nSimilar to our multi-document QA results, GPT-\\n3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\\nInstruct have the lowest performance when they\\nmust access key-value pairs in the middle of their\\ninput context. LongChat-13B (16K) exhibits a dif-']\n",
      "\n",
      "3. Q: How much worse is the middle really? Is it noticeable?\n",
      "   Answer: Based on the context provided, the middle model has a dramatic primacy and recency bias, with a 20-point accuracy disparity between the best- and worst-case performance. This difference is significant and noticeable. Even with additional fine-tuning, the bias remains significant.\n",
      "   Reference: Very noticeable! The paper shows accuracy can drop from ~90% for information at the start to ~70% in the middle, then back up to ~85% at the end. That's a 20+ percentage point swing just from position. For production systems, this is a critical performance hit.\n",
      "   Contexts: ['bias (10-point worst-case degradation), but the bias\\nremains significant. However, the 70B models\\nwith and without additional fine-tuning have largely\\nsimilar trends (showing both primacy and recency\\nbias), and additional fine-tuning minimally changes\\nthe positional bias severity.', 'a dramatic primacy and recency bias—there is a\\n20-point accuracy disparity between the best- and\\nworst-case performance. Applying additional fine-\\ntuning to the 13B seems to slightly reduce this\\nbias (10-point worst-case degradation), but the bias\\nremains significant. However, the 70B models']\n",
      "\n",
      "4. Q: Does this happen with all models or just GPT?\n",
      "   Answer: According to the context, this issue of performance degradation when relevant information occurs in the middle of the input context is not specific to just GPT-4. Other models such as GPT-3.5-Turbo, GPT-3.5-Turbo (16K), MPT-30B-Instruct, and LongChat-13B (16K) also experience this issue.\n",
      "   Reference: All models tested! The paper evaluated GPT-3.5, GPT-4, Claude, and open-source models. They all show the U-shaped performance curve. This appears to be a fundamental limitation of how causal language models are trained, not specific to one provider.\n",
      "   Contexts: ['claude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\ngpt-4-0613\\nFigure 15: Although GPT-4 has higher absolute perfor-\\nmance than other models, its performance still degrades\\nwhen relevant information occurs in the middle of the\\ninput context.', 'context, not all models achieve high performance.\\nSimilar to our multi-document QA results, GPT-\\n3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30B-\\nInstruct have the lowest performance when they\\nmust access key-value pairs in the middle of their\\ninput context. LongChat-13B (16K) exhibits a dif-']\n",
      "\n",
      "5. Q: What if I have 10 chunks? Where should I put them?\n",
      "   Answer: Based on the context provided, if you have 10 chunks of information, you should place them in the input context in the middle. This is because models like LongChat-13B (16K) tend to retrieve information from the middle of the input context rather than at the start. Placing the chunks in the middle will help the model generate code to retrieve the key effectively.\n",
      "   Reference: Rank by relevance, then arrange: [1st, 10th, 9th, 4th, 5th, 6th, 7th, 8th, 3rd, 2nd]. Put your best chunks at positions 1, 2, 9, 10. Put your worst chunks in positions 4-7 (the dead zone). This maximizes the chance your important information gets used.\n",
      "   Contexts: ['is a paragraph (as opposed to a list or a table). We\\nuse passages (chunks of at most 100 tokens) from\\nWikipedia as documents within our input contexts.\\nFor each of the queries, we need a document\\nthat contains the answer and k − 1 distractor\\ndocuments that do not contain the answer. To', 'models must retrieve from the middle of the input context.\\nplaced at the start of the input context, LongChat-\\n13B (16K) tends to generate code to retrieve the\\nkey, rather than outputting the value directly.\\n4 Why Are Language Models Not Robust\\nto Changes in the Position of Relevant\\nInformation?']\n",
      "\n",
      "6. Q: Can I just tell the model \"pay attention to the middle\"?\n",
      "   Answer: No, based on the context provided, it is not sufficient to just tell the model to \"pay attention to the middle\" because language models like LongChat-13B (16K) tend to generate code to retrieve the key when placed at the start of the input context, rather than outputting the value directly. Additionally, the performance of these models is higher when relevant information occurs at the beginning or end of the input context, as opposed to the middle.\n",
      "   Reference: The paper tested this - it doesn't work well. You can add instructions like \"carefully read all information\" but models still show position bias. The issue seems to be in the attention mechanism itself, not just following instructions. Better to work with the bias than fight it.\n",
      "   Contexts: ['models must retrieve from the middle of the input context.\\nplaced at the start of the input context, LongChat-\\n13B (16K) tends to generate code to retrieve the\\nkey, rather than outputting the value directly.\\n4 Why Are Language Models Not Robust\\nto Changes in the Position of Relevant\\nInformation?', 'these models are evaluated on sequences longer than those seen during training (center and right subplots), we\\nobserve a U-shaped performance curve—performance is higher when relevant information occurs at the beginning\\nor end of the input context, as opposed to the middle of the input context.']\n",
      "\n",
      "7. Q: What's the minimum context length where this matters?\n",
      "   Answer: The minimum context length where the performance of using information within long input contexts is minimally affected by the position of the relevant information is not specified in the given context.\n",
      "   Reference: The effect appears around 2k+ tokens. For very short contexts (<1k tokens), position matters less. But once you're retrieving 3+ decent-sized chunks (300-500 tokens each), you're in the danger zone and need to think about positioning.\n",
      "   Contexts: ['robustly use information within long input con-\\ntexts, it is necessary to show that its performance\\nis minimally affected by the position of the rele-\\nvant information in the input context (e.g., minimal\\ndifference in best- and worst-case performance).', 'the beginning of the input context) when prompted\\nwith instruction-formatted data. We hypothesize\\nthat non-instruction fine-tuned language models\\nlearn to use these long contexts from similarly-\\nformatted data that may occur in Internet text seen\\nduring pre-training, e.g., StackOverflow questions']\n",
      "\n",
      "8. Q: Does this apply to my prompt instructions too?\n",
      "   Answer: Based on the context provided, it seems that the effectiveness of fine-tuning language models on a dataset of instructions and responses may vary based on where the task specification or instructions are placed within the input context. If the relevant information is at the very beginning of the context, randomizing the distractor order may slightly decrease performance. Therefore, it is possible that the placement of instructions in your prompt may impact the performance of fine-tuning language models.\n",
      "   Reference: Yes! If you have a long system prompt, put your most important instructions at the beginning and end. Don't bury critical guidelines in the middle of a 1000-word instruction set. Many people put key rules at the top as a header for this reason.\n",
      "   Contexts: ['supervised fine-tuning on a dataset of instructions\\nand responses. The task specification and/or in-\\nstruction is commonly placed at the beginning of\\nthe input context in supervised instruction fine-\\ntuning data, which might lead instruction fine-\\ntuned language models to place more weight on', 'input contexts. Comparing the results in §2.3 with\\nthose when randomizing the distractor order and\\nmentioning such in the prompt, we see that ran-\\ndomization slightly decreases performance when\\nthe relevant information is at the very beginning\\nof the context, and slightly increases performance']\n",
      "\n",
      "9. Q: What about question-answering? Should I put the question at the start or end?\n",
      "   Answer: Based on the context provided, it is suggested that for question-answering, it is best to put the question at the start or end of the input context. Performance is highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models must reason over information in the middle of their input context.\n",
      "   Reference: The paper suggests putting the question at the END after all context. This mirrors how humans read - consume information, then answer the question. Pattern: [Context chunks positioned strategically] + [Question at end].\n",
      "   Contexts: ['document question answering performance. Lower positions are closer to the start of the input context. Performance\\nis highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models\\nmust reason over information in the middle of their input context.', 'single document that contains the answer and must\\nuse it to answer the question.\\nModel performance is highest when relevant in-\\nformation occurs at the beginning or end of its\\ninput context. As illustrated in Figure 5, chang-\\ning the position of relevant information in the in-']\n",
      "\n",
      "10. Q: Is there a sweet spot for number of chunks?\n",
      "   Answer: Based on the given data, there does not seem to be a clear sweet spot for the number of chunks. The average total retrieved documents and maximum values vary across different models and number of chunks, with fluctuations in performance.\n",
      "   Reference: 3-5 chunks is optimal. With 3 chunks, you have: [important] [less important] [important]. With 5: [best] [good] [mediocre] [good] [best]. Beyond 5-7 chunks, you create a larger \"dead zone\" in the middle that will be ignored.\n",
      "   Contexts: ['50\\n55\\n60\\n65\\n70Accuracy\\n20 T otal Retrieved Documents (~4K tokens)\\n1st 5th 10th 15th 20th 25th 30th\\nPosition of Document with the Answer\\n50\\n55\\n60\\n65\\n70Accuracy\\n30 T otal Retrieved Documents (~6K tokens)\\nmpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2', '10 docs 20 docs 30 docs\\navg ± stdev max avg ± stdev max avg ± stdev max\\nLongChat-13B (16K) 1749.9 ± 112.4 2511 3464.6 ± 202.3 4955 5181.9 ± 294.7 7729\\nMPT-30B 1499.7 ± 88.5 1907 2962.4 ± 158.4 3730 4426.9 ± 230.5 5475\\nGPT-3.5-Turbo 1475.6 ± 86.5 1960 2946.2 ± 155.1 3920 4419.2 ± 226.5 6101']\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvaluationResult' object has no attribute 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Reference: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Contexts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRAGas scores:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mscores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'EvaluationResult' object has no attribute 'results'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# Initialize the LLM for evaluation\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # sync interface compatible with RAGas\n",
    "\n",
    "# Run evaluation\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llm,  # pass the LLM explicitly\n",
    "    num_samples=1\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation dataset preview:\")\n",
    "for i, row in enumerate(evaluation_dataset):\n",
    "    print(f\"{i+1}. Q: {row['question']}\")\n",
    "    print(f\"   Answer: {row['answer']}\")\n",
    "    print(f\"   Reference: {row['reference']}\")\n",
    "    print(f\"   Contexts: {row['contexts']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a1aff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS scores:\n",
      " Answer correctness: mean=0.3471, std=0.1708\n",
      " Answer relevancy:   mean=0.6882, std=0.3478\n",
      " Faithfulness:       mean=0.6012, std=0.2875\n",
      " Context precision:  mean=0.7500, std=0.3354\n",
      " Context recall:     mean=0.3917, std=0.3576\n"
     ]
    }
   ],
   "source": [
    "# print RAGAS result\n",
    "print(\n",
    "    f\"RAGAS scores:\"\n",
    "    f\"\\n Answer correctness: mean={np.mean(scores['answer_correctness']):.4f}, std={np.std(scores['answer_correctness']):.4f}\"\n",
    "    f\"\\n Answer relevancy:   mean={np.mean(scores['answer_relevancy']):.4f}, std={np.std(scores['answer_relevancy']):.4f}\"\n",
    "    f\"\\n Faithfulness:       mean={np.mean(scores['faithfulness']):.4f}, std={np.std(scores['faithfulness']):.4f}\"\n",
    "    f\"\\n Context precision:  mean={np.mean(scores['context_precision']):.4f}, std={np.std(scores['context_precision']):.4f}\"\n",
    "    f\"\\n Context recall:     mean={np.mean(scores['context_recall']):.4f}, std={np.std(scores['context_recall']):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ee04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Running config: chunk=300, overlap=0, model=text-embedding-3-small, top_k=2\n",
      "Split 18 pages into 244 chunks.\n",
      "\n",
      "Saved 244 chunks to chroma.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n  Value error, Found n supplied twice. [type=value_error, input_value={'model': 'gpt-3.5-turbo'...model_kwargs': {'n': 3}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m evaluation_dataset = Dataset.from_list(evaluation_rows)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# sync interface compatible with RAGas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m llm = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Explicitly pass to model kwargs)  \u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m     53\u001b[39m scores = evaluate(\n\u001b[32m     54\u001b[39m     evaluation_dataset,\n\u001b[32m     55\u001b[39m     metrics=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     llm=llm,  \u001b[38;5;66;03m# pass the LLM explicitly\u001b[39;00m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\pydantic\\main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatOpenAI\n  Value error, Found n supplied twice. [type=value_error, input_value={'model': 'gpt-3.5-turbo'...model_kwargs': {'n': 3}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/papers\"    # LLMs paper\n",
    "\n",
    "# Example parameter grid\n",
    "# chunk_sizes = [256, 512, 1024]\n",
    "# chunk_overlaps = [0, 50]\n",
    "# embedding_models = [\"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
    "# retriever_top_k = [2, 5]\n",
    "\n",
    "chunk_sizes = [300]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "embedding_models = [\"text-embedding-3-small\"]\n",
    "retriever_top_k = [2]\n",
    "\n",
    "results = []\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "for chunk_size, overlap, emb_model, top_k in itertools.product(chunk_sizes, chunk_overlaps, embedding_models, retriever_top_k):\n",
    "    print(f\"\\n🔧 Running config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "\n",
    "    # --- Step 1: Build your retriever & RAG pipeline ---\n",
    "    chunks = split_text(documents, chunk_size, overlap)\n",
    "    db = save_to_chroma(chunks, CHROMA_PATH, emb_model)\n",
    "\n",
    "    # generate answer in evaluation format and change it to dataset\n",
    "    qa_file = \"data/papers/Beginner Q&A_eval.txt\"\n",
    "    evaluation_rows = generate_answer_in_eval_format(qa_file, db, top_k)\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "\n",
    "    # Run evaluation\n",
    "    scores = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,  # pass the LLM explicitly\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Collect results ---\n",
    "    results.append({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"embedding_model\": emb_model,\n",
    "        \"top_k\": top_k,\n",
    "        \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "        \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "        \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "        \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "        \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "        \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "        \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "        \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "        \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "        \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "    })\n",
    "\n",
    "# --- Step 4: Save and inspect ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ragas_auto_eval_results.csv\", index=False)\n",
    "print(\"\\n✅ Evaluation complete! Results saved to ragas_auto_eval_results.csv\")\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
