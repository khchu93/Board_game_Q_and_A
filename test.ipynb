{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9741f3",
   "metadata": {},
   "source": [
    "### Building a Local Knowledge Assistant with LangChain & OpenAI\n",
    "\n",
    "In this project, I implemented a `Retrieval-Augmented Generation (RAG)` pipeline using `LangChain`, `Chroma`, and `OpenAI embeddings`. The system retrieves relevant chunks of information from my document collection to provide more accurate and context-aware answers. By combining retrieval with generation, I can leverage a smaller, pre-trained LLM while still achieving detailed and precise responses, without the need for costly fine-tuning. This setup allows me to experiment with AI-driven Q&A in a practical, hands-on way.\n",
    "\n",
    "This project was inspired by [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch?v=tcqEUSNCn8I) and adapted for personal learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf581640",
   "metadata": {},
   "source": [
    "### Why RAG Matters?\n",
    "\n",
    "As language models grow larger, updating and controlling their knowledge becomes increasingly impractical through fine-tuning alone. `RAG` introduces a retrieval-based paradigm that preserves model generality while enabling precise, real-time knowledge integration. &nbsp;&nbsp; â‡¾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8965fb",
   "metadata": {},
   "source": [
    "**Traditional Approaches** <br>\n",
    "**`Fine-tuning`** works but comes with serious baggage: massive **compute requirements**, maintaining multiple **model versions**, knowledge that goes **stale** (requiring full retraining), and catastrophic **forgetting** where models lose general capabilities. **`LoRA`** makes this more efficient by training **only low-rank matrices** instead of full weights, but you're still **burning GPU hours**, tuning **hyperparameters**, andâ€”criticallyâ€”**baking knowledge** into opaque model weights. Want to update a single fact? Good luck auditing which training examples influenced what, or making changes without retraining from scratch.\n",
    "\n",
    "**RAG: Knowledge Without Retraining** <br>\n",
    "`RAG` flips the script. Instead of modifying the model, it retrieves **relevant context** from **vector databases at inference time**. The advantages are game-changing: **instant knowledge updates** without retraining, **traceable citations** grounding outputs in source material, and **zero training compute**â€”just embed and index your documents. Your knowledge stays **external** and **auditable**, perfect for rapidly changing domains like customer support or compliance where information updates weekly. Plus, you get clean separation of concerns: **the LLM handles reasoning while retrieval manages knowledge**.\n",
    "\n",
    "**Trade-offs** <br>\n",
    "The catch? You're now in the **retrieval** game. Chunking strategies, embedding quality, and similarity metrics directly impact what context reaches your model. Poor retrieval means **hallucinations**. **Context windows** limit how much you can include, requiring **smart ranking** when relevant info spans many documents. But these challenges beat managing expensive retraining pipelinesâ€”RAG trades GPU clusters for vector database tuning, a much more tractable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47d63f",
   "metadata": {},
   "source": [
    "### Why LangChain?\n",
    "\n",
    "LLM applications in production need to integrate with dozens of **servicesâ€”vector databases**, **document loaders**, **model providers**, **monitoring tools**. Alternatives like `Mirascope` or `LiteLLM` focus on **specific integration** challenges, but `LangChain`'s ecosystem is unmatched: **100+ integrations** maintained by both the core team and community, reducing custom boilerplate significantly.  &nbsp;&nbsp; â‡¾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225221e2",
   "metadata": {},
   "source": [
    "**Going Solo: Reinventing the Wheel** <br>\n",
    "**Going solo means building infrastructure from scratch**. `Raw API` calls seem simple until you're managing conversation history, implementing retry logic, handling streaming responses, and coordinating multi-step chains. **Custom frameworks give you control but demand serious engineering**: designing abstractions, integrating new model providers, building observability, maintaining compatibility. Every `RAG` pipeline or `agent system` requires **solving the same orchestration problems** teams everywhere have already tackled.\n",
    "\n",
    "**What LangChain Brings to the Table** <br>\n",
    "`LangChain` consolidates years of collective learning into **reusable patterns**. The framework shines in three areas: **provider abstraction** (switch between OpenAI, Anthropic, local models without rewriting logic), **composable chains** (LCEL makes complex workflows readable and debuggable), and **ecosystem integrations** (vector stores, document loaders, agent toolsâ€”all plug-and-play). You're not just getting codeâ€”you're getting architectural patterns that scale, from simple prompt chains to sophisticated agent loops with memory and tool use.\n",
    "\n",
    "**What You're Trading For: Velocity vs Transparency** <br>\n",
    "The tradeoff is **framework dependency**. `LangChain` adds abstraction layers that can **obscure what's happening under the hood**. Simple tasks might feel overengineered, and rapid framework evolution means staying current with breaking changes. Debugging requires understanding LangChain's execution model, not just your application logic. But the math usually works out: teams shipping complex LLM features fast versus teams stuck building plumbing. For most production use cases, LangChain's velocity wins outweigh the framework overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef501fb",
   "metadata": {},
   "source": [
    "### Why Chroma?\n",
    "\n",
    "When working on LLM projects that involve `semantic search` or `RAG`, vector databases are essential for storing and querying embeddings efficiently. `Chroma` stands out because it **balances ease-of-use, flexibility, and functionality**. Unlike `FAISS` (a low-level similarity search library), `Pinecone` (managed cloud service), or `Weaviate` (complex self-hosted solution), `Chroma` lets you run embedded in **your application** or as a **standalone server**, supports metadata and persistence out-of-the-box, and integrates seamlessly with frameworks like LangChainâ€”making it ideal for developers who want to move fast without infrastructure overhead. &nbsp;&nbsp; â‡¾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b1427",
   "metadata": {},
   "source": [
    "**The Vector Database Landscape** <br>\n",
    "Many other solutions have their trade-offs. `FAISS` is extremely fast at large-scale similarity search but requires **manual management of metadata and persistence**. `Pinecone` offers a fully managed solution with advanced features but comes with **ongoing costs** and **cloud-only** limitations. `Weaviate` provides hybrid search and ML model integration but has a **steeper learning curve** and more **complex setup**. For engineers, these hurdles can slow down project development.\n",
    "\n",
    "**Chroma: Simplicity Without Sacrifice** <br>\n",
    "`Chroma`, on the other hand, simplifies the workflow. Its Python API is intuitive, it integrates naturally with `LangChain`, and it supports both **embedded** and **client-server deployment** modes. You can store embeddings along with metadata, filter results easily, and get your RAG pipelines running quickly. For small-to-medium scale projects, it allows you to focus on building intelligent LLM applications rather than spending time on database plumbing.\n",
    "\n",
    "**Where Chroma Makes Trade-offs** <br>\n",
    "Of course, `Chroma` has limitations. It **isn't optimized for extremely large datasets** with hundreds of millions of vectors and lacks some of the **advanced indexing options** that `FAISS` offers. However, for engineers who want to rapidly prototype and deploy LLM-powered solutions, `Chroma` provides a perfect balance of simplicity, power, and flexibility to get real results without unnecessary friction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Adapted from \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "# https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "import openai \n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca6c1",
   "metadata": {},
   "source": [
    "The project begins by **converting the target documents into LangChain** `Document` **objects** using the `DirectoryLoader` function. This preserves both the **text** and **metadata**, such as the source path and start index, which are required for LangChain's downstream functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw files into Document\n",
    "def load_documents(DATA_PATH: str):\n",
    "    # loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")  # Alice in the wonderland\n",
    "    # documents = loader.load()\n",
    "    documents = []\n",
    "    for pdf_file in glob.glob(f\"{DATA_PATH}/*.pdf\"):\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e062cf",
   "metadata": {},
   "source": [
    "Next, the documents are **split into smaller chunks**. This is necessary primarily for **retrieval precision in RAG systems**. Smaller chunks allow the system to retrieve **only the most relevant information** rather than entire documents, improving semantic similarity matching. **Large chunks** can **dilute semantic meaning** and introduce **irrelevant context**, while **smaller chunks** enable more **focused retrieval**. Additionally, chunking helps manage **LLM context window limitations**, though retrieval quality is the primary consideration.\n",
    "\n",
    "The 'RecursiveCharacterTextSplitter' controls how text is divided using two key parameters: 'chunk_size' and 'chunk_overlap'. 'chunk_size' defines the *maximum length* of each chunk, while 'chunk_overlap' *repeats a portion* of the previous chunk in the next one to **reduce the chance of splitting related information** across chunk boundaries. Choosing appropriate values for these parameters directly impacts retrieval quality, and thus the overall performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into small chunks\n",
    "def split_text(documents: list[Document], chunk_size: int=300, chunk_overlap: int=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,          #nb of characters in each chunk\n",
    "        chunk_overlap=chunk_overlap,    #nb of characters to overlap between chunks\n",
    "        length_function=len,            #decide how to measure the chunk, e.g., character, token, etc\n",
    "        add_start_index=True,           #add the starting index of the chunk\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} pages into {len(chunks)} chunks.\\n\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d6bb",
   "metadata": {},
   "source": [
    "Once the documents are split, each chunk is converted into a **vector embedding** using OpenAI's 'text-embedding-3-small' model. These **embeddings** are then stored in a 'Chroma' vector database along with the **original chunk text and metadata**. The 'Chroma' database enables efficient **similarity search** by **comparing query embeddings against stored chunk embeddings**.\n",
    "\n",
    "It is crucial to use the **same embedding model** for both **indexing chunks** and **embedding queries** because different models produce **incompatible vector representations** that exist in different semantic spaces, making retrieval unreliable or impossible. For this project, I use the default embedding model `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vector embedding to chunks and save the embedding vector along with the content and metadata to database\n",
    "def save_to_chroma(chunks: list[Document], CHROMA_PATH: str, model=\"text-embedding-ada-002\"):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(model=model), persist_directory=None\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c29e2",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b93f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 48 pages into 620 chunks.\n",
      "\n",
      "Print chunk 10 content: \n",
      "Content: \"3\n",
      "CN3081CATAN5 of 5\n",
      "v6.250401\n",
      "LARGEST ARMY\n",
      "LONGEST ROUTE\n",
      "The first player to play 3 Knight cards receives this tile. If another player plays more Knight cards, they immediately receive this tile.\"\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2025-03-05T13:56:17-06:00', 'moddate': '2025-03-05T13:56:17-06:00', 'source': 'data/BoardGamesRuleBook\\\\CATAN.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'start_index': 0}\n",
      "\n",
      "Saved 620 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "# DATA_PATH = \"data/books\"    # Alice in the wonderland\n",
    "# DATA_PATH = \"data/papers\"    # LLMs paper\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"    # LLMs paper\n",
    "\n",
    "# split the document in chunks and save it to database along with its embedded vector\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_text(documents)\n",
    "\n",
    "print(f\"Print chunk 10 content: \")\n",
    "document = chunks[10]\n",
    "print(f\"Content: \\\"{document.page_content}\\\"\")\n",
    "print(f\"Metadata: {document.metadata}\\n\")\n",
    "\n",
    "db = save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac1a",
   "metadata": {},
   "source": [
    "The **prompt template** determines how **retrieved context** and the **user query** are **structured** for the **language model**. While prompt quality is difficult to quantify precisely, adhering to **established prompting principles significantly improves model responses**. Effective LLM prompting requires **clear, specific instructions** - similar to providing detailed directions to a new team member. Well-defined prompts guide the model toward desired outputs, while vague or ambiguous instructions increase output unpredictability, often resulting in irrelevant or inaccurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ed89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d87c",
   "metadata": {},
   "source": [
    "Once the Chroma database and prompt template are configured, the system can process user queries through the following pipeline: First, the query is embedded using the **same embedding model** applied to the document chunks, ensuring vector space consistency. The system then retrieves the **top-k most similar chunks** from the vector database (using L2 (Euclidean) distance by default). These retrieved chunks are combined with the original query according to the prompt template structure and passed to the LLM (default LLM: `gpt-3.5-turbo`) for response generation.\n",
    "\n",
    "Implementing **quality safeguards** is essential for production systems. This includes **rejecting empty** or **malformed queries** and filtering results when similarity scores fall **below a confidence threshold**, as low-similarity retrievals typically indicate insufficient relevant context and lead to unreliable outputs. With these components in place, the RAG pipeline can effectively retrieve pertinent information and generate well-informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1bfcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: str, isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in context])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28aae",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17466451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 similarity:\n",
      "Top 1:\n",
      "Content: does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "L2 similarity: 0.7823288076491218\n",
      "\n",
      "Top 2:\n",
      "Content: If a player has more than one building on the new hex, only steal 1 card. You may only play this \n",
      "card after the first barbarian attack (when the robber is placed on the board).\n",
      "Â© 2025 CATAN GmbH\n",
      "VICTORY POINT\n",
      "CONSTITUTION\n",
      "Play immediately into your  \n",
      "player area, even if it is  \n",
      "not your turn.\n",
      "L2 similarity: 0.7805705254229444\n",
      "\n",
      "Top 3:\n",
      "Content: You may not move the pirate until you have at least one route \n",
      "between one of your buildings and a village on a small island. When \n",
      "you move the pirate, you may either steal a resource card or a cloth \n",
      "token from a player whose ship is on an edge of the pirateâ€™s new hex.\n",
      "WINNING THE GAME\n",
      "L2 similarity: 0.7649600913777823\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "\n",
      "---\n",
      "\n",
      "If a player has more than one building on the new hex, only steal 1 card. You may only play this \n",
      "card after the first barbarian attack (when the robber is placed on the board).\n",
      "Â© 2025 CATAN GmbH\n",
      "VICTORY POINT\n",
      "CONSTITUTION\n",
      "Play immediately into your  \n",
      "player area, even if it is  \n",
      "not your turn.\n",
      "\n",
      "---\n",
      "\n",
      "You may not move the pirate until you have at least one route \n",
      "between one of your buildings and a village on a small island. When \n",
      "you move the pirate, you may either steal a resource card or a cloth \n",
      "token from a player whose ship is on an edge of the pirateâ€™s new hex.\n",
      "WINNING THE GAME\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Can you steal from a city or settlement you just built this turn?\n",
      "\n",
      "Response: No, you cannot steal from a city or settlement you just built this turn.\n",
      "Sources: ['data/BoardGamesRuleBook\\\\CATAN_Cities&Knights.pdf', 'data/BoardGamesRuleBook\\\\CATAN_Cities&Knights.pdf', 'data/BoardGamesRuleBook\\\\CATAN_Seafarers.pdf']\n"
     ]
    }
   ],
   "source": [
    "# query_text = input(\"Enter your query: \")\n",
    "query_text = \"Can you steal from a city or settlement you just built this turn?\"\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results, similarity too low.\")\n",
    "    # return\n",
    "\n",
    "print(\"Top 3 similarity:\")\n",
    "for i, k in enumerate(results):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(f\"Top {i + 1}:\\nContent: {k[0].page_content}\\nL2 similarity: {k[1]}\\n\")\n",
    "\n",
    "response_text = generate_answer(query_text, results, True)\n",
    "\n",
    "# Print the formatted response\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text.content}\\nSources: {sources}\"\n",
    "print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722ea44",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "`RAG` evaluation breaks down into two critical stages: **retrieval** and **generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d92424",
   "metadata": {},
   "source": [
    "**Retrieval Evaluation**<br>\n",
    "In retrieval evaluation, we **measure how effectively the system identifies and ranks relevant documents for a given query**. Standard metrics like `Precision@k`, `Recall@k`, and `Mean Reciprocal Rank (MRR)` quantify whether the retriever surfaces useful chunks in its top results. High precision means most retrieved chunks are relevant; high recall means we're not missing critical information. These metrics guide optimization of chunk size, overlap, and embedding model selectionâ€”essentially tuning what context reaches the generator.\n",
    "\n",
    "**Generation Evaluation**<br>\n",
    "Generation evaluation **assesses the quality and factual grounding of responses given the retrieved context**. We use `RAGAS (Retrieval-Augmented Generation Assessment)` metrics: `faithfulness` (does the answer align with retrieved evidence?), answer correctness (how close to ground truth?), `answer relevancy` (is it on-topic?), and `context precision/recall` (did we retrieve the right supporting passages?). These metrics reveal not just what the model generates, but how well it uses retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fbc06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content from Q&A file generated with ChatGPT\n",
    "# def read_qa_file_by_paper(file_path: str):\n",
    "def read_qa_file(file_path: str):\n",
    "    rows = []\n",
    "    current_game = None\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "            # if line.lower().startswith(\"paper:\"):\n",
    "            #     current_paper = line[len(\"paper:\"):].strip()\n",
    "            if line.lower().startswith(\"board_game:\"):\n",
    "                current_game = line[len(\"board_game:\"):].strip()\n",
    "            elif line.lower().startswith(\"q:\"):\n",
    "                question = line[len(\"q:\"):].strip()\n",
    "            elif line.lower().startswith(\"a:\"):\n",
    "                answer = line[len(\"a:\"):].strip()\n",
    "                if current_game is None:\n",
    "                    raise ValueError(f\"Answer found without specifying board game: {line}\")\n",
    "                if question is None:\n",
    "                    raise ValueError(f\"Answer found without a question: {line}\")\n",
    "                rows.append({\n",
    "                    \"board_game\": current_game,\n",
    "                    \"question\": question,\n",
    "                    \"reference\": answer\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(f\"Line format not recognized: {line}\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d91211a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_in_eval_format(path: str, chroma: Chroma, k: int):\n",
    "    evaluation_rows = []\n",
    "\n",
    "    # Extract Q&A content from txt\n",
    "    qa_rows = read_qa_file(path)\n",
    "\n",
    "    for row in qa_rows:\n",
    "        question = row[\"question\"]\n",
    "        ground_truth = row[\"reference\"]\n",
    "        board_game = row[\"board_game\"]\n",
    "\n",
    "        # Retrieve context from database and generate the answer\n",
    "        context = chroma.similarity_search_with_relevance_scores(question, k=k)\n",
    "        answer = generate_answer(question, context)\n",
    "\n",
    "        # Organize the information into the format required by RAGAS\n",
    "        evaluation_rows.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": [doc.page_content for doc, _score in context],  # <-- plain text\n",
    "            # \"answer\": answer_with_source,\n",
    "            \"answer\": answer.content,\n",
    "            \"reference\": ground_truth,\n",
    "            \"source\": board_game\n",
    "        })\n",
    "    return evaluation_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cc0b3",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf25307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate answer in evaluation format\n",
    "# qa_file = \"data/papers/Beginner Q&A.txt\"\n",
    "qa_file = \"data/BoardGamesRuleBook/CATAN_Q&A.txt\"\n",
    "evaluation_rows = generate_answer_in_eval_format(path=qa_file, chroma=db, k=2)\n",
    "\n",
    "# change it to Dataset format\n",
    "evaluation_dataset = Dataset.from_list(evaluation_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49982d1e",
   "metadata": {},
   "source": [
    "**Why RAGAS?** <br>\n",
    "Traditional NLP metrics like `BLEU` or `ROUGE` measure surface-level text similarity but ignore whether answers are grounded in evidenceâ€”critical for `RAG` systems where factual accuracy and traceability matter. `RAGAS` provides task-aware, automated evaluation that directly **measures how generated content connects to retrieved context**. Its modular design integrates seamlessly with `LangChain` and `Weights & Biases`, enabling scalable experimentation and real-time tracking. Essentially, `RAGAS` lets us **evaluate retrieval-generation alignment** systematically, leading to more interpretable and reliable `RAG` performance insights without constant human annotation.\n",
    "\n",
    "**The limitations?** <br>\n",
    "`RAGAS` relies on **LLM-as-judge evaluation**, meaning it **uses language models (often GPT-4) to score metrics** like `faithfulness` and `relevancy`. This introduces **cost** (API calls for every evaluation), **latency** (slower than traditional metrics), and **potential bias** (the judge model's own limitations affect scores). Additionally, `RAGAS` metrics **require ground truth datasets** for answer correctness, which aren't always available or easy to create for domain-specific applications. The framework also has a **learning curve**â€”understanding what each metric measures and how to interpret scores together requires familiarity with `RAG`-specific evaluation concepts. Despite these trade-offs, for most `RAG` applications, `RAGAS`'s ability to measure grounding and factuality outweighs the operational overhead compared to purely manual evaluation or metrics that ignore retrieval quality entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcd22998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/385 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   0%|          | 1/385 [00:03<24:28,  3.83s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 2/385 [00:08<26:02,  4.08s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 3/385 [00:26<1:07:40, 10.63s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   5%|â–Œ         | 20/385 [01:10<18:59,  3.12s/it] LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   5%|â–Œ         | 21/385 [01:24<32:32,  5.36s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   6%|â–‹         | 25/385 [01:31<20:20,  3.39s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   8%|â–Š         | 31/385 [01:46<16:39,  2.82s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  10%|â–ˆ         | 39/385 [02:04<14:31,  2.52s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  11%|â–ˆâ–        | 44/385 [02:17<14:45,  2.60s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  12%|â–ˆâ–        | 45/385 [02:25<19:52,  3.51s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  14%|â–ˆâ–Ž        | 52/385 [02:45<15:57,  2.88s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  14%|â–ˆâ–        | 55/385 [02:52<13:33,  2.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  16%|â–ˆâ–Œ        | 61/385 [03:14<19:21,  3.58s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  19%|â–ˆâ–‰        | 75/385 [03:47<09:38,  1.87s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  21%|â–ˆâ–ˆâ–       | 82/385 [04:09<11:17,  2.23s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 85/385 [04:19<12:52,  2.58s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 87/385 [04:24<12:13,  2.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 90/385 [04:32<11:26,  2.33s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  25%|â–ˆâ–ˆâ–       | 96/385 [04:50<11:38,  2.42s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 103/385 [05:12<10:00,  2.13s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  29%|â–ˆâ–ˆâ–Š       | 110/385 [05:34<12:31,  2.73s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  29%|â–ˆâ–ˆâ–‰       | 113/385 [05:48<15:41,  3.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  30%|â–ˆâ–ˆâ–‰       | 114/385 [05:53<17:56,  3.97s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 122/385 [06:12<10:27,  2.38s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 129/385 [06:31<10:54,  2.56s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 132/385 [06:42<12:37,  2.99s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 135/385 [06:48<11:14,  2.70s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 146/385 [07:20<10:18,  2.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 147/385 [07:29<16:23,  4.13s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 154/385 [07:46<10:37,  2.76s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 164/385 [08:12<07:08,  1.94s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 165/385 [08:15<08:26,  2.30s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 172/385 [08:35<07:37,  2.15s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 177/385 [08:52<10:03,  2.90s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 181/385 [09:07<10:34,  3.11s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 184/385 [09:16<10:52,  3.25s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 190/385 [09:35<09:52,  3.04s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 200/385 [09:59<05:49,  1.89s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 219/385 [10:59<05:58,  2.16s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 222/385 [11:05<05:15,  1.93s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 228/385 [11:25<06:14,  2.39s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 244/385 [12:17<07:44,  3.29s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 255/385 [12:48<05:46,  2.66s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 268/385 [13:25<05:29,  2.81s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 281/385 [13:56<03:24,  1.97s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 287/385 [14:21<05:17,  3.24s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 294/385 [14:36<03:28,  2.29s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 305/385 [15:08<02:39,  2.00s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 306/385 [15:25<05:58,  4.54s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 326/385 [16:17<03:07,  3.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 337/385 [16:46<02:19,  2.91s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 346/385 [17:22<03:33,  5.49s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 360/385 [18:05<01:32,  3.72s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 366/385 [18:14<00:38,  2.05s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 379/385 [18:48<00:15,  2.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 385/385 [19:02<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset preview:\n",
      "1. Q: What is needed to buy a development card?\n",
      "   Answer: To buy a development card, you need to build or move one of your ships to an edge with a development card.\n",
      "   Reference: 1 wool, 1 grain, and 1 ore.\n",
      "   Contexts: ['DEVELOPMENT CARDS\\nWhen you build or move one of your ships to an edge with a development card, \\ntake the card. Use it as if you bought it this turn (i.e., you may play it on a future \\nturn or immediately reveal it to win the game).\\nPORTS', 'You may play 1 development card during your turn by placing it face up in your player area.  \\nIt may not be a card you built this turn. You may play a development card before rolling dice \\nor at any time during the Action phase.']\n",
      "\n",
      "2. Q: Can you play multiple development cards on the same turn?\n",
      "   Answer: No, you can only play one development card per turn according to the context provided.\n",
      "   Reference: No, only one development card per turn (except Victory Points).\n",
      "   Contexts: ['PLAY DEVELOPMENT CARDS\\nâ€¢ Not on the turn you bought it\\nâ€¢ Only 1 development card per turn\\nâ€¢ Before rolling dice or during the Action phase\\nVP card exception: You may play multiple VP cards (even on the \\nturn you buy them) in order to win the game.\\nCN3081\\nCATAN\\n3 of 5\\nv6.250401\\nVP0\\nVP1\\nVPs2\\nVPs?\\nVP0', 'You may play 1 development card during your turn by placing it face up in your player area.  \\nIt may not be a card you built this turn. You may play a development card before rolling dice \\nor at any time during the Action phase.']\n",
      "\n",
      "3. Q: How are roads placed?\n",
      "   Answer: Players place roads on edges with a check mark, avoiding edges where they may not build due to other players' settlements.\n",
      "   Reference: Each road must connect to one of your existing roads, settlements, or cities.\n",
      "   Contexts: ['Example: Blue may place a road on any of the \\nedges with a check mark. They may not place \\na road on the edge with an â€œXâ€ because they \\nmay not build on the other side of  \\nOrangeâ€™s settlement.', 'the same until all players have 1 settlement and 1 road on the board.\\nROUND 2\\nStarting with the last player and going in reverse order, each player places 1 city on an \\nempty intersection of their choice and their second road on an empty adjacent edge.']\n",
      "\n",
      "4. Q: Can a player refuse all trade offers?\n",
      "   Answer: Yes, a player can refuse all trade offers if they do not find any proposed trades beneficial to them.\n",
      "   Reference: Yes, all trades between players are voluntary.\n",
      "   Contexts: ['to trade. Other players may accept your proposal, make counteroffers, or make their own proposals.\\nImportant: You may not give away cards in any way, which includes trading matching resource cards \\n(for example, trying to trade 3 ore for 1 ore is not allowed). \\nPORT TRADE WITH THE SUPPL Y', 'There are three types of trades you may perform:\\nTRADE WITH OTHER PLAYERS\\nTo trade with other players, announce which resource(s) you want and which resource(s) you are willing \\nto trade. Other players may accept your proposal, make counteroffers, or make their own proposals.']\n",
      "\n",
      "5. Q: What happens when a player rolls a 7?\n",
      "   Answer: When a player rolls a 7, players with more than 7 resource cards lose half of them as normal, and then the player who rolled the 7 may steal from other players.\n",
      "   Reference: No resources are produced, and the robber is moved to a new hex.\n",
      "   Contexts: ['If you are stronger, receive 1 resource card of your choice \\nfrom the supply.\\nIf you are tied with the pirate, nothing happens.\\nRESOLVING A 7\\nWhen a 7 is rolled, players with \\nmore than 7 resource cards lose half \\nof them as normal. Then the player \\nwho rolled the 7 may steal', 'in your hand.\\n6   Choose the First Player \\nEach player rolls the dice. The player with the \\nhighest roll is the first player.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# Initialize the LLM for evaluation\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # sync interface compatible with RAGas\n",
    "\n",
    "# Run evaluation\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llm,  # pass the LLM explicitly\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation dataset preview:\")\n",
    "for i, row in enumerate(evaluation_dataset):\n",
    "    if i >= 5: #only print the first 5 Q&A result\n",
    "        break\n",
    "    print(f\"{i+1}. Q: {row['question']}\")\n",
    "    print(f\"   Answer: {row['answer']}\")\n",
    "    print(f\"   Reference: {row['reference']}\")\n",
    "    print(f\"   Contexts: {row['contexts']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a1aff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS scores:\n",
      " Answer correctness: mean=0.4701, std=0.2603\n",
      " Answer relevancy:   mean=0.9503, std=0.1192\n",
      " Faithfulness:       mean=0.4361, std=0.3741\n",
      " Context precision:  mean=0.6948, std=0.4199\n",
      " Context recall:     mean=0.7403, std=0.4385\n"
     ]
    }
   ],
   "source": [
    "# print RAGAS result\n",
    "print(\n",
    "    f\"RAGAS scores:\"\n",
    "    f\"\\n Answer correctness: mean={np.mean(scores['answer_correctness']):.4f}, std={np.std(scores['answer_correctness']):.4f}\"\n",
    "    f\"\\n Answer relevancy:   mean={np.mean(scores['answer_relevancy']):.4f}, std={np.std(scores['answer_relevancy']):.4f}\"\n",
    "    f\"\\n Faithfulness:       mean={np.mean(scores['faithfulness']):.4f}, std={np.std(scores['faithfulness']):.4f}\"\n",
    "    f\"\\n Context precision:  mean={np.mean(scores['context_precision']):.4f}, std={np.std(scores['context_precision']):.4f}\"\n",
    "    f\"\\n Context recall:     mean={np.mean(scores['context_recall']):.4f}, std={np.std(scores['context_recall']):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3ee04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Running config: chunk=300, overlap=0, model=text-embedding-3-small, top_k=2\n",
      "Split 18 pages into 244 chunks.\n",
      "\n",
      "Saved 244 chunks to chroma.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n  Value error, Found n supplied twice. [type=value_error, input_value={'model': 'gpt-3.5-turbo'...model_kwargs': {'n': 3}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m evaluation_dataset = Dataset.from_list(evaluation_rows)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# sync interface compatible with RAGas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m llm = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Explicitly pass to model kwargs)  \u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m     53\u001b[39m scores = evaluate(\n\u001b[32m     54\u001b[39m     evaluation_dataset,\n\u001b[32m     55\u001b[39m     metrics=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     llm=llm,  \u001b[38;5;66;03m# pass the LLM explicitly\u001b[39;00m\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:116\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\pydantic\\main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatOpenAI\n  Value error, Found n supplied twice. [type=value_error, input_value={'model': 'gpt-3.5-turbo'...model_kwargs': {'n': 3}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"    # LLMs paper\n",
    "\n",
    "# Example parameter grid\n",
    "# chunk_sizes = [256, 512, 1024]\n",
    "# chunk_overlaps = [0, 50]\n",
    "# embedding_models = [\"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
    "# retriever_top_k = [2, 5]\n",
    "\n",
    "chunk_sizes = [300]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "embedding_models = [\"text-embedding-3-small\"]\n",
    "retriever_top_k = [2]\n",
    "\n",
    "results = []\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "for chunk_size, overlap, emb_model, top_k in itertools.product(chunk_sizes, chunk_overlaps, embedding_models, retriever_top_k):\n",
    "    print(f\"\\nðŸ”§ Running config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "\n",
    "    # --- Step 1: Build your retriever & RAG pipeline ---\n",
    "    chunks = split_text(documents, chunk_size, overlap)\n",
    "    db = save_to_chroma(chunks, CHROMA_PATH, emb_model)\n",
    "\n",
    "    # generate answer in evaluation format and change it to dataset\n",
    "    qa_file = \"data/BoardGamesRuleBook/CATAN_Q&A_Eval.txt\"\n",
    "    evaluation_rows = generate_answer_in_eval_format(qa_file, db, top_k)\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "\n",
    "    # Run evaluation\n",
    "    scores = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,  # pass the LLM explicitly\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Collect results ---\n",
    "    results.append({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"embedding_model\": emb_model,\n",
    "        \"top_k\": top_k,\n",
    "        \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "        \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "        \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "        \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "        \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "        \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "        \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "        \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "        \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "        \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "    })\n",
    "\n",
    "# --- Step 4: Save and inspect ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ragas_auto_eval_results.csv\", index=False)\n",
    "print(\"\\nâœ… Evaluation complete! Results saved to ragas_auto_eval_results.csv\")\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
